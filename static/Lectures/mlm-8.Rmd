---
title: "MLM"
author: Josh Jackson
date: "11-24-20"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)

library(tidyverse)
library(broom)
library(tidybayes)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small{ 
  font-size: 80%}
.tiny{
  font-size: 65%}
</style>

## This time

MLM bayes Style

---
## MLM review 

$${Y}_{i} = b_{0} + b_{1}X_{i} +  ... +\epsilon_{i}$$

$${Y}_{ij} = b_{0} + b_{1}X_{ij} + ... +\epsilon_{i}$$
Where j refers to some clustering or grouping variable and i refers to the observations within j

```{r, echo = FALSE, message = FALSE, warning = FALSE}
data <- "https://raw.githubusercontent.com/josh-jackson/bayes/master/mlm.csv"

mlm <- read.csv(data) 

```


```{r, echo = FALSE}
library(tidyverse)
sample_n_of <- function(data, size, ...) {
  dots <- quos(...)
  
  group_ids <- data %>% 
    group_by(!!! dots) %>% 
    group_indices()
  
  sampled_groups <- sample(unique(group_ids), size)
  data %>% 
    filter(group_ids %in% sampled_groups)
}
```

---
```{r, echo = FALSE, message = FALSE, warning = FALSE}

set.seed(114)
mlm %>%
  sample_n_of(20, ID) %>% 
ggplot(aes(x = week, y = CON, group = ID, fill = ID)) + geom_point(aes(color = factor(ID))) + stat_smooth(aes(color = factor(ID)),method = "lm", se = FALSE) +
  xlab("X") + ylab("Y") + theme(legend.position = "none")

```


---
### Empty model

Level 1
$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$

Level 2
$${\beta}_{0j} = \gamma_{00} + U_{0j}$$

$${U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})$$
$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$

---


  
.pull-left[
```{r, echo = FALSE, warning = FALSE}

mlm %>%
  sample_n_of(8, ID) %>% 
ggplot(aes(x = week, y = CON, group = ID)) + geom_point(aes(color = factor(ID))) + stat_smooth(aes(color = factor(ID)), method = "lm", formula=y~1, se = FALSE) + xlab("X") + ylab("Y") + theme(legend.position = "none")
```
]

.pull-right[
  $${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$

Akin to ANOVA if we treat $U_{0j}$ as between subjects variance & $\varepsilon_{ij}$ as within subjects variance. 
]

---
## Random and fixed effects

.pull-left[
Level 1:
$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$

Level 2:
$${\beta}_{0j} = \gamma_{00} + U_{0j}$$
Combined:
$${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$
]

.pull-right[
$U_{0j}$ is considered a random effect, as it is varies across our grouping

$\gamma_{00}$ is considered a fixed effect, as it is what is fixed (average) across our grouping
]

---
## Level 1 predictors

Level 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. Is your cluster people? Then observations are level 1. 

$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$
Notice on the subscript of X that these predictors vary across group (j) and within the group (i) So if your grouping (j) is people, then i refers to different observations. 

---

Level 2 takes the parameters at level 1 and decomposes them into a fixed component ($\gamma$) that reflects the average and, if desired, the individual deviations around that fixed effect (U).

level 1
$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$

level 2
$${\beta}_{0j} = \gamma_{00} + U_{0j}$$  
$${\beta}_{1j} = \gamma_{10}$$  

---

By including random effects (U) you making a claim that every group/cluster does *not* have the same $\gamma$ ie intercept/regression coefficient. 

An advantage of MLM is to separate more "buckets" of variance that are unexplained. What was originally $e_ij$ is now ( $U_0$ + $U_1$ + $e_ij$ ). This additional decomposition of variance is beneficial because you are separating signal from noise, translating what was noise $e_ij$ into meaningful signal ( $U_0$ + $U_1$ ). 

For example, multiple responses per person can identify individual differences (eg not everyone shows the stroop effect) that normally would be chalked up to error. If you parse out this error your signal becomes stronger. 

---
## Predictions for a person
.pull-left[
Level 1:
 
$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$
Level 2:  
$${\beta}_{0j} = \gamma_{00} + U_{0j}$$


$${\beta}_{1j} = \gamma_{10} + U_{1j}$$  

Combined
$${Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij}$$
]
.pull-right[
Can think of a persons score divided up into a fixed component as well as the random component. 

$${\beta}_{16} = \gamma_{10} \pm U_{16}$$ 
]


---
## Error structure
The residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance. 

$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} & \tau_{01}\\ 
  0,  \tau_{01} & \tau_{10}^{2}
\end{pmatrix}$$

Note that it is possible to have a different error structure for the random effects

$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$

---
## Multiple level 1 predictors

Level 1:
 
$${Y}_{ijk} = \beta_{0j}  + \beta_{1j}X_{ij} + \beta_{2j}Z_{ik} + \varepsilon_{ijk}$$
Level 2:  
$${\beta}_{0j} = \gamma_{00} + U_{0j}$$

$${\beta}_{1j} = \gamma_{10} + U_{1j}$$ 
$${\beta}_{2k} = \gamma_{20} + U_{2k}$$

---
## Level 2 predictors
.small[
Level 1: 
$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$
Level 2: 
$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$  
$${\beta}_{1j} = \gamma_{10} + U_{1j}$$  
]

Combined
  $${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$
  $${Y}_{ij} = [\gamma_{00} + \gamma_{01}G_{j}+ U_{0j}]  + [(\gamma_{10}  + U_{1j})(Time_{ij})] + \varepsilon_{ij}$$

---
## Cross level interactions

.small[
level 1: 
$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$
Level 2: 
$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$  
$${\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j}$$  
]

Combined
  $${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + \gamma_{11}(G_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$

$${Y}_{ij} = [\gamma_{00} + U_{0j} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j}+  U_{1j})(Time_{ij})] + \varepsilon_{ij}$$

---
## Centering

As a rule, each level-1 predictor is usually really 2 predictor variables. It is important to separate within-group from between group variance. Failing to do so will "smush" between and within variance to level 1.

Example: student SES at level 1, with schools at level 2. 
Some kids have more money than other kids in their school
Some schools have more money than other schools

Fortunately it is easy to separate this

---

Level 1: 
$${Y}_{ij} = \beta_{0j}  + \beta_{1j}(X_{ij}- \bar{X_j}) + \varepsilon_{ij}$$

Level 2: 
$${\beta}_{0j} = \gamma_{00} + \bar{X_j} +   U_{0j}$$ 

$${\beta}_{1j} = \gamma_{10} + \bar{X_j} + U_{1j}$$ 

---
## MLM as default and make it maximum

If you have data like this you should analyze like this! If you don't, then you are losing information (GEE being a potential exception).

The question often is about which random effects to fit. There are multiple opinions, but mine is to start maximum, and then work downward (ie simplify) if necessary/consistent with theory. It doesn't really hurt to include them, especially within Bayesian estimation! 

---
## Partial pooling/shrinkage

We treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions. 

We do this in standard regression where we make predictions based on values not only using data from X but from the whole dataset. A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data! 


---
## priors for priors

The extent we do the shrinkage depends on a few things (eg number of observations per group) but also depends on our priors. Also known as hyperpriors

$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$
$$\mu_i  = \beta_{0[i]}$$ 

$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma)$$
$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$
$$\sigma \sim {\operatorname{Exponential}(1)}$$

---

```{r, echo = FALSE, message = FALSE}
library(brms)
```


```{r}
mlm.1 <- 
  brm(family = gaussian,
      CON ~ 1 + (1 | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = sd), 
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.1")
```

---

```{r}
summary(mlm.1)
```

---
Notice we have one less paramter we are estimating. 
```{r}
mlm.2 <- 
  brm(family = gaussian,
      CON ~ 1 ,
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.2")
```

---
```{r}
summary(mlm.2)
```

---
```{r}
mlm.1 <- add_criterion(mlm.1, "loo")
mlm.2 <- add_criterion(mlm.2, "loo")
loo_compare(mlm.1, mlm.2, criterion = "loo")
```


---
```{r, echo = FALSE, warning = FALSE}

mlm %>%
  sample_n_of(8, ID) %>% 
ggplot(aes(x = week, y = CON, group = ID)) + geom_point(aes(color = factor(ID))) + stat_smooth(aes(color = factor(ID)), method = "lm", formula=y~1, se = FALSE) + xlab("X") + ylab("Y") + theme(legend.position = "none")
```

---
When we have these priors it indicates that we have some 
$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$
$$\mu_i  = \beta_{0[i]}$$ 
This line indicates we have varying intercepts. Much like going from level 1 -> 2, we do priors on that parameter. Indicates that there is a distribution FOR EACH PERSON/GROUP. The distribution needs a prior. 
$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma)$$
$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$
$$\sigma \sim {\operatorname{Exponential}(1)}$$

---
## Random slopes

$$y_i \sim \text{Normal}(\mu, \sigma)$$ 
$$\mu_i = \beta_{0,\text{pid}} + \beta_{1, \text{pid}}x_i$$
$$
[\beta_{0,\text{pid}}
\beta_{1, \text{pid}}]
\sim \text{MVNormal}\bigg(
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
, \Sigma
\bigg)$$

$$\Sigma = 
\left(\begin{array}{cc}
\sigma_{\beta_0}&0\\
0&\sigma_{\beta_1}
\end{array}\right)\Omega
\left(\begin{array}{cc}
\sigma_{\beta_0}&0\\
0&\sigma_{\beta_1}
\end{array}\right)$$

$$\beta_0 \sim \text{Normal}(0, 1)$$
$$\beta_1 \sim \text{Normal}(0, 1)$$
$$\sigma_{\beta_0} \sim \text{Exponential}(1)$$
$$\sigma_{\beta_1} \sim \text{Exponential}(1)$$
$$\sigma \sim \text{Exponential}(1)$$
$$\Omega \sim \text{LKJcorr(2)}$$
