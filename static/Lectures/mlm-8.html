<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MLM</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="mlm-8_files/header-attrs-2.5/header-attrs.js"></script>
    <link href="mlm-8_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="mlm-8_files/anchor-sections-1.0/anchor-sections.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small{ 
  font-size: 80%}
.tiny{
  font-size: 65%}
&lt;/style&gt;

## This time

MLM bayes Style

---
## MLM review 

`$${Y}_{i} = b_{0} + b_{1}X_{i} +  ... +\epsilon_{i}$$`

`$${Y}_{ij} = b_{0} + b_{1}X_{ij} + ... +\epsilon_{i}$$`
Where j refers to some clustering or grouping variable and i refers to the observations within j






---
![](mlm-8_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
### Empty model

Level 1
`$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$`

Level 2
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`

`$${U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})$$`
`$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$`

---


  
.pull-left[
![](mlm-8_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]

.pull-right[
  `$${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$`

Akin to ANOVA if we treat `\(U_{0j}\)` as between subjects variance &amp; `\(\varepsilon_{ij}\)` as within subjects variance. 
]

---
## Random and fixed effects

.pull-left[
Level 1:
`$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$`

Level 2:
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`
Combined:
`$${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$`
]

.pull-right[
`\(U_{0j}\)` is considered a random effect, as it is varies across our grouping

`\(\gamma_{00}\)` is considered a fixed effect, as it is what is fixed (average) across our grouping
]

---
## Level 1 predictors

Level 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. Is your cluster people? Then observations are level 1. 

`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Notice on the subscript of X that these predictors vary across group (j) and within the group (i) So if your grouping (j) is people, then i refers to different observations. 

---

Level 2 takes the parameters at level 1 and decomposes them into a fixed component ($\gamma$) that reflects the average and, if desired, the individual deviations around that fixed effect (U).

level 1
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`

level 2
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10}$$`  

---

By including random effects (U) you making a claim that every group/cluster does *not* have the same `\(\gamma\)` ie intercept/regression coefficient. 

An advantage of MLM is to separate more "buckets" of variance that are unexplained. What was originally `\(e_ij\)` is now ( `\(U_0\)` + `\(U_1\)` + `\(e_ij\)` ). This additional decomposition of variance is beneficial because you are separating signal from noise, translating what was noise `\(e_ij\)` into meaningful signal ( `\(U_0\)` + `\(U_1\)` ). 

For example, multiple responses per person can identify individual differences (eg not everyone shows the stroop effect) that normally would be chalked up to error. If you parse out this error your signal becomes stronger. 

---
## Predictions for a person
.pull-left[
Level 1:
 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2:  
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`


`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$`  

Combined
`$${Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij}$$`
]
.pull-right[
Can think of a persons score divided up into a fixed component as well as the random component. 

`$${\beta}_{16} = \gamma_{10} \pm U_{16}$$` 
]


---
## Error structure
The residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance. 

`$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} &amp; \tau_{01}\\ 
  0,  \tau_{01} &amp; \tau_{10}^{2}
\end{pmatrix}$$`

Note that it is possible to have a different error structure for the random effects

`$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$`

---
## Multiple level 1 predictors

Level 1:
 
`$${Y}_{ijk} = \beta_{0j}  + \beta_{1j}X_{ij} + \beta_{2j}Z_{ik} + \varepsilon_{ijk}$$`
Level 2:  
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`

`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$` 
`$${\beta}_{2k} = \gamma_{20} + U_{2k}$$`

---
## Level 2 predictors
.small[
Level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$`  
]

Combined
  `$${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$`
  `$${Y}_{ij} = [\gamma_{00} + \gamma_{01}G_{j}+ U_{0j}]  + [(\gamma_{10}  + U_{1j})(Time_{ij})] + \varepsilon_{ij}$$`

---
## Cross level interactions

.small[
level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j}$$`  
]

Combined
  `$${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + \gamma_{11}(G_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$`

`$${Y}_{ij} = [\gamma_{00} + U_{0j} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j}+  U_{1j})(Time_{ij})] + \varepsilon_{ij}$$`

---
## Centering

As a rule, each level-1 predictor is usually really 2 predictor variables. It is important to separate within-group from between group variance. Failing to do so will "smush" between and within variance to level 1.

Example: student SES at level 1, with schools at level 2. 
Some kids have more money than other kids in their school
Some schools have more money than other schools

Fortunately it is easy to separate this

---

Level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}(X_{ij}- \bar{X_j}) + \varepsilon_{ij}$$`

Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \bar{X_j} +   U_{0j}$$` 

`$${\beta}_{1j} = \gamma_{10} + \bar{X_j} + U_{1j}$$` 

---
## MLM as default and make it maximum

If you have data like this you should analyze like this! If you don't, then you are losing information (GEE being a potential exception).

The question often is about which random effects to fit. There are multiple opinions, but mine is to start maximum, and then work downward (ie simplify) if necessary/consistent with theory. It doesn't really hurt to include them, especially within Bayesian estimation! 

---
## Partial pooling/shrinkage

We treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions. 

We do this in standard regression where we make predictions based on values not only using data from X but from the whole dataset. A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data! 


---
## priors for priors

The extent we do the shrinkage depends on a few things (eg number of observations per group) but also depends on our priors. Also known as hyperpriors

`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$`
`$$\mu_i  = \beta_{0[i]}$$` 

`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma_0 \sim {\operatorname{Exponential}(1)}$$`
`$$\sigma \sim {\operatorname{Exponential}(1)}$$`

---





```r
mlm.1 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + (1 | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = sd), 
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.1")
```

---


```r
summary(mlm.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + (1 | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.01     0.05     0.07 1.00     5205     8457
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.20 1.00     8238    10695
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     9478    11175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
Notice we have one less paramter we are estimating. 

```r
mlm.2 &lt;- 
  brm(family = gaussian,
      CON ~ 1 ,
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.2")
```

---

```r
summary(mlm.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.00     0.18     0.20 1.00    16473    11582
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.07      0.00     0.07     0.08 1.00     9374     9244
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
mlm.1 &lt;- add_criterion(mlm.1, "loo")
mlm.2 &lt;- add_criterion(mlm.2, "loo")
loo_compare(mlm.1, mlm.2, criterion = "loo")
```

```
##       elpd_diff se_diff
## mlm.1   0.0       0.0  
## mlm.2 -58.6       8.8
```


---
![](mlm-8_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

When we have these priors it indicates that we have some 
`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma)$$`
`$$\mu_i  = \beta_{0[i]}$$` 
This line indicates we have varying intercepts. Much like going from level 1 -&gt; 2, we do priors on that parameter. Indicates that there is a distribution FOR EACH PERSON/GROUP. The distribution needs a prior. 
`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma_0 \sim {\operatorname{Exponential}(1)}$$`
`$$\sigma \sim {\operatorname{Exponential}(1)}$$`



---
## Ways to think about MLM

1. Multiple levels of focus (L1 vs L2, within vs between, trial vs person)
2. Creating piles of meaningful variance

3. Parameters that depend on parameters
4. Shrinkage towards mean of some population (higher N, less shrinkage)


---
### Parameters that depend on parameters

![](mlm-8_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;


---
###  Shrinkage towards mean 

.small[
`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$`

`$$\mu_i  = \beta_{0[i]}$$` 

`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`

`$$\sigma_0 \sim {\operatorname{Normal}(0, 1.5)}$$`



`$$\sigma \sim {\operatorname{Exponential}(1)}$$`
]

Half normal can turn on/off shrinkage. Setting SD to zero says everyone is the same (complete). Infinite SD says all groups are unrelated (no pooling). In the middle gives you partial pooling. Remember, with MLE, you are using flat priors.   

---


```r
summary(mlm.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + (1 | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.01     0.05     0.07 1.00     5205     8457
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.20 1.00     8238    10695
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     9478    11175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```



---


```r
library(tidybayes)
get_variables(mlm.1)
```

```
##   [1] "b_Intercept"         "sd_ID__Intercept"    "sigma"              
##   [4] "r_ID[6,Intercept]"   "r_ID[29,Intercept]"  "r_ID[34,Intercept]" 
##   [7] "r_ID[36,Intercept]"  "r_ID[37,Intercept]"  "r_ID[48,Intercept]" 
##  [10] "r_ID[53,Intercept]"  "r_ID[54,Intercept]"  "r_ID[58,Intercept]" 
##  [13] "r_ID[61,Intercept]"  "r_ID[66,Intercept]"  "r_ID[67,Intercept]" 
##  [16] "r_ID[69,Intercept]"  "r_ID[71,Intercept]"  "r_ID[74,Intercept]" 
##  [19] "r_ID[75,Intercept]"  "r_ID[76,Intercept]"  "r_ID[78,Intercept]" 
##  [22] "r_ID[79,Intercept]"  "r_ID[80,Intercept]"  "r_ID[81,Intercept]" 
##  [25] "r_ID[82,Intercept]"  "r_ID[85,Intercept]"  "r_ID[86,Intercept]" 
##  [28] "r_ID[87,Intercept]"  "r_ID[89,Intercept]"  "r_ID[91,Intercept]" 
##  [31] "r_ID[92,Intercept]"  "r_ID[93,Intercept]"  "r_ID[94,Intercept]" 
##  [34] "r_ID[96,Intercept]"  "r_ID[97,Intercept]"  "r_ID[98,Intercept]" 
##  [37] "r_ID[99,Intercept]"  "r_ID[101,Intercept]" "r_ID[102,Intercept]"
##  [40] "r_ID[103,Intercept]" "r_ID[104,Intercept]" "r_ID[105,Intercept]"
##  [43] "r_ID[106,Intercept]" "r_ID[110,Intercept]" "r_ID[112,Intercept]"
##  [46] "r_ID[114,Intercept]" "r_ID[115,Intercept]" "r_ID[116,Intercept]"
##  [49] "r_ID[120,Intercept]" "r_ID[122,Intercept]" "r_ID[125,Intercept]"
##  [52] "r_ID[127,Intercept]" "r_ID[129,Intercept]" "r_ID[135,Intercept]"
##  [55] "r_ID[136,Intercept]" "r_ID[137,Intercept]" "r_ID[140,Intercept]"
##  [58] "r_ID[141,Intercept]" "r_ID[142,Intercept]" "r_ID[143,Intercept]"
##  [61] "r_ID[144,Intercept]" "r_ID[146,Intercept]" "r_ID[149,Intercept]"
##  [64] "r_ID[150,Intercept]" "r_ID[152,Intercept]" "r_ID[153,Intercept]"
##  [67] "r_ID[155,Intercept]" "r_ID[156,Intercept]" "r_ID[159,Intercept]"
##  [70] "r_ID[160,Intercept]" "r_ID[162,Intercept]" "r_ID[163,Intercept]"
##  [73] "r_ID[165,Intercept]" "r_ID[167,Intercept]" "r_ID[169,Intercept]"
##  [76] "r_ID[171,Intercept]" "r_ID[174,Intercept]" "r_ID[182,Intercept]"
##  [79] "r_ID[187,Intercept]" "r_ID[189,Intercept]" "r_ID[190,Intercept]"
##  [82] "r_ID[193,Intercept]" "r_ID[194,Intercept]" "r_ID[201,Intercept]"
##  [85] "r_ID[204,Intercept]" "r_ID[205,Intercept]" "r_ID[208,Intercept]"
##  [88] "r_ID[209,Intercept]" "r_ID[211,Intercept]" "r_ID[214,Intercept]"
##  [91] "r_ID[219,Intercept]" "r_ID[222,Intercept]" "r_ID[223,Intercept]"
##  [94] "r_ID[229,Intercept]" "prior_Intercept"     "prior_sigma"        
##  [97] "prior_sd_ID"         "lp__"                "accept_stat__"      
## [100] "stepsize__"          "treedepth__"         "n_leapfrog__"       
## [103] "divergent__"         "energy__"
```


---


```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) 
```

```
## # A tibble: 1,456,000 x 6
## # Groups:   ID, term [91]
##       ID term          r_ID .chain .iteration .draw
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;
##  1     6 Intercept  0.00585      1          1     1
##  2     6 Intercept  0.00850      1          2     2
##  3     6 Intercept  0.00333      1          3     3
##  4     6 Intercept  0.00222      1          4     4
##  5     6 Intercept  0.0231       1          5     5
##  6     6 Intercept  0.0147       1          6     6
##  7     6 Intercept  0.00593      1          7     7
##  8     6 Intercept  0.00779      1          8     8
##  9     6 Intercept -0.00722      1          9     9
## 10     6 Intercept  0.00491      1         10    10
## # … with 1,455,990 more rows
```
16000 samples (4 chains * 4k iterations) * 91 people

---


```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) %&gt;%
 median_qi()
```

```
## # A tibble: 91 x 8
## # Groups:   ID [91]
##       ID term           r_ID   .lower   .upper .width .point .interval
##    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
##  1     6 Intercept  0.000352 -0.0483   0.0496    0.95 median qi       
##  2    29 Intercept -0.0479   -0.105    0.00966   0.95 median qi       
##  3    34 Intercept -0.0611   -0.118   -0.00271   0.95 median qi       
##  4    36 Intercept -0.0224   -0.0794   0.0357    0.95 median qi       
##  5    37 Intercept -0.00303  -0.0515   0.0454    0.95 median qi       
##  6    48 Intercept  0.0231   -0.0250   0.0720    0.95 median qi       
##  7    53 Intercept  0.000113 -0.0581   0.0581    0.95 median qi       
##  8    54 Intercept  0.0453   -0.00385  0.0932    0.95 median qi       
##  9    58 Intercept  0.0367   -0.0140   0.0871    0.95 median qi       
## 10    61 Intercept -0.0746   -0.131   -0.0176    0.95 median qi       
## # … with 81 more rows
```


---
.pull-left[

```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) %&gt;%
  left_join(mlm %&gt;% select (ID, CON) %&gt;% group_by(ID) %&gt;%  mutate(CON = mean(CON))) %&gt;% 
 median_qi() %&gt;% 
ggplot(aes(x = ID)) +
  geom_point(aes(y = r_ID), color = "orange2") +
  geom_point(aes(y = CON), shape = 1) 
```
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
]

---


```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) 
```

```
## # A tibble: 1,456,000 x 7
## # Groups:   ID, term [91]
##    .chain .iteration .draw b_Intercept    ID term          r_ID
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1      1          1     1       0.185     6 Intercept  0.00585
##  2      1          1     1       0.185    29 Intercept -0.0423 
##  3      1          1     1       0.185    34 Intercept -0.149  
##  4      1          1     1       0.185    36 Intercept  0.0127 
##  5      1          1     1       0.185    37 Intercept  0.0299 
##  6      1          1     1       0.185    48 Intercept  0.0213 
##  7      1          1     1       0.185    53 Intercept  0.00556
##  8      1          1     1       0.185    54 Intercept  0.0314 
##  9      1          1     1       0.185    58 Intercept  0.0559 
## 10      1          1     1       0.185    61 Intercept -0.0675 
## # … with 1,455,990 more rows
```

---


```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) %&gt;% 
   mutate(person_I = b_Intercept + r_ID) 
```

```
## # A tibble: 1,456,000 x 8
## # Groups:   ID, term [91]
##    .chain .iteration .draw b_Intercept    ID term          r_ID person_I
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;
##  1      1          1     1       0.185     6 Intercept  0.00585   0.191 
##  2      1          1     1       0.185    29 Intercept -0.0423    0.143 
##  3      1          1     1       0.185    34 Intercept -0.149     0.0363
##  4      1          1     1       0.185    36 Intercept  0.0127    0.198 
##  5      1          1     1       0.185    37 Intercept  0.0299    0.215 
##  6      1          1     1       0.185    48 Intercept  0.0213    0.206 
##  7      1          1     1       0.185    53 Intercept  0.00556   0.191 
##  8      1          1     1       0.185    54 Intercept  0.0314    0.216 
##  9      1          1     1       0.185    58 Intercept  0.0559    0.241 
## 10      1          1     1       0.185    61 Intercept -0.0675    0.118 
## # … with 1,455,990 more rows
```

---

![](mlm-8_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---
.pull-left[

```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) %&gt;%
   median_qi(person_I = b_Intercept + r_ID) %&gt;%
  ggplot(aes( y = reorder(ID, person_I), x = person_I, xmin = .lower, xmax = .upper)) +
  geom_pointinterval() + xlab("est") + ylab("ID")
```
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]

---
## Random slopes

`$$y_i \sim \text{Normal}(\mu, \sigma)$$` 
`$$\mu_i = \beta_{0j} + \beta_{1j}X_{ij}$$`
$$
[\beta_{0j},
\beta_{1j}]
\sim \text{MVNormal}
`\begin{bmatrix}
\beta_0 \\
\beta_1\end{bmatrix}, \Sigma\bigg)$$`


`$$\Sigma = 
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;0\\
0&amp;\sigma_{\beta_1}
\end{array}\right)\Omega
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;0\\
0&amp;\sigma_{\beta_1}
\end{array}\right)$$`

`$$\beta_0 \sim \text{Normal}(0, 1)$$`
`$$\beta_1 \sim \text{Normal}(0, 1)$$`
`$$\sigma_{\beta_0} \sim \text{Exponential}(1)$$`
`$$\sigma_{\beta_1} \sim \text{Exponential}(1)$$`
`$$\sigma \sim \text{Exponential}(1)$$`
`$$\Omega \sim \text{LKJcorr(2)}$$`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
