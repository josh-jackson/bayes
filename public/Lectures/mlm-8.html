<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MLM</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="mlm-8_files/header-attrs-2.5/header-attrs.js"></script>
    <link href="mlm-8_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="mlm-8_files/anchor-sections-1.0/anchor-sections.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small{ 
  font-size: 80%}
.tiny{
  font-size: 65%}
&lt;/style&gt;

## This time

MLM bayes Style

---
## MLM review 

`$${Y}_{i} = b_{0} + b_{1}X_{i} +  ... +\epsilon_{i}$$`

`$${Y}_{ij} = b_{0} + b_{1}X_{ij} + ... +\epsilon_{i}$$`
Where j refers to some clustering or grouping variable and i refers to the observations within j






---
![](mlm-8_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
### Empty model

Level 1
`$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$`

Level 2
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`

`$${U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})$$`
`$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$`

---


  
.pull-left[
![](mlm-8_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]

.pull-right[
  `$${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$`

Akin to ANOVA if we treat `\(U_{0j}\)` as between subjects variance &amp; `\(\varepsilon_{ij}\)` as within subjects variance. 
]

---
## Random and fixed effects

.pull-left[
Level 1:
`$${Y}_{ij} = \beta_{0j}  + \varepsilon_{ij}$$`

Level 2:
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`
Combined:
`$${Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij}$$`
]

.pull-right[
`\(U_{0j}\)` is considered a random effect, as it is varies across our grouping

`\(\gamma_{00}\)` is considered a fixed effect, as it is what is fixed (average) across our grouping
]

---
## Level 1 predictors

Level 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. Is your cluster people? Then observations are level 1. 

`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Notice on the subscript of X that these predictors vary across group (j) and within the group (i) So if your grouping (j) is people, then i refers to different observations. 

---

Level 2 takes the parameters at level 1 and decomposes them into a fixed component ($\gamma$) that reflects the average and, if desired, the individual deviations around that fixed effect (U).

level 1
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`

level 2
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10}$$`  

---

By including random effects (U) you making a claim that every group/cluster does *not* have the same `\(\gamma\)` ie intercept/regression coefficient. 

An advantage of MLM is to separate more "buckets" of variance that are unexplained. What was originally `\(e_ij\)` is now ( `\(U_0\)` + `\(U_1\)` + `\(e_ij\)` ). This additional decomposition of variance is beneficial because you are separating signal from noise, translating what was noise `\(e_ij\)` into meaningful signal ( `\(U_0\)` + `\(U_1\)` ). 

For example, multiple responses per person can identify individual differences (eg not everyone shows the stroop effect) that normally would be chalked up to error. If you parse out this error your signal becomes stronger. 

---
## Predictions for a person
.pull-left[
Level 1:
 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2:  
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`


`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$`  

Combined
`$${Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij}$$`
]
.pull-right[
Can think of a persons score divided up into a fixed component as well as the random component. 

`$${\beta}_{16} = \gamma_{10} \pm U_{16}$$` 
]


---
## Error structure
The residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance. 

`$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} &amp; \tau_{01}\\ 
  0,  \tau_{01} &amp; \tau_{10}^{2}
\end{pmatrix}$$`

Note that it is possible to have a different error structure for the random effects

`$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$`

---
## Multiple level 1 predictors

Level 1:
 
`$${Y}_{ijk} = \beta_{0j}  + \beta_{1j}X_{ij} + \beta_{2j}Z_{ik} + \varepsilon_{ijk}$$`
Level 2:  
`$${\beta}_{0j} = \gamma_{00} + U_{0j}$$`

`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$` 
`$${\beta}_{2k} = \gamma_{20} + U_{2k}$$`

---
## Level 2 predictors
.small[
Level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10} + U_{1j}$$`  
]

Combined
  `$${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$`
  `$${Y}_{ij} = [\gamma_{00} + \gamma_{01}G_{j}+ U_{0j}]  + [(\gamma_{10}  + U_{1j})(Time_{ij})] + \varepsilon_{ij}$$`

---
## Cross level interactions

.small[
level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j}$$`  
]

Combined
  `$${Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (X_{ij}) + \gamma_{11}(G_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij}$$`

`$${Y}_{ij} = [\gamma_{00} + U_{0j} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j}+  U_{1j})(Time_{ij})] + \varepsilon_{ij}$$`

---
## Centering

As a rule, each level-1 predictor is usually really 2 predictor variables. It is important to separate within-group from between group variance. Failing to do so will "smush" between and within variance to level 1.

Example: student SES at level 1, with schools at level 2. 
Some kids have more money than other kids in their school
Some schools have more money than other schools

Fortunately it is easy to separate this

---

Level 1: 
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}(X_{ij}- \bar{X_j}) + \varepsilon_{ij}$$`

Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \bar{X_j} +   U_{0j}$$` 

`$${\beta}_{1j} = \gamma_{10} + \bar{X_j} + U_{1j}$$` 

---
## MLM as default and make it maximum

If you have data like this you should analyze like this! If you don't, then you are losing information (GEE being a potential exception).

The question often is about which random effects to fit. There are multiple opinions, but mine is to start maximum, and then work downward (ie simplify) if necessary/consistent with theory. It doesn't really hurt to include them, especially within Bayesian estimation! 

---
## Partial pooling/shrinkage

We treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions. 

We do this in standard regression where we make predictions based on values not only using data from X but from the whole dataset. A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data! 


---
## priors for priors

The extent we do the shrinkage depends on a few things (eg number of observations per group) but also depends on our priors. Also known as hyperpriors

`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$`
`$$\mu_i  = \beta_{0[i]}$$` 

`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma_0 \sim {\operatorname{Exponential}(1)}$$`
`$$\sigma \sim {\operatorname{Exponential}(1)}$$`

---





```r
mlm.1 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + (1 | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = sd), 
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.1")
```

---


```r
summary(mlm.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + (1 | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.01     0.05     0.07 1.00     5205     8457
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.20 1.00     8238    10695
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     9478    11175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
Notice we have one less paramter we are estimating. 

```r
mlm.2 &lt;- 
  brm(family = gaussian,
      CON ~ 1 ,
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sigma)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      data = mlm, 
      file = "mlm.2")
```

---

```r
summary(mlm.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.00     0.18     0.20 1.00    16473    11582
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.07      0.00     0.07     0.08 1.00     9374     9244
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
mlm.1 &lt;- add_criterion(mlm.1, "loo")
mlm.2 &lt;- add_criterion(mlm.2, "loo")
loo_compare(mlm.1, mlm.2, criterion = "loo")
```

```
##       elpd_diff se_diff
## mlm.1   0.0       0.0  
## mlm.2 -58.6       8.8
```


---
![](mlm-8_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

When we have these priors it indicates that we have some 
`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma)$$`
`$$\mu_i  = \beta_{0[i]}$$` 
This line indicates we have varying intercepts. Much like going from level 1 -&gt; 2, we do priors on that parameter. Indicates that there is a distribution FOR EACH PERSON/GROUP. The distribution needs a prior. 
`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma_0 \sim {\operatorname{Exponential}(1)}$$`
`$$\sigma \sim {\operatorname{Exponential}(1)}$$`



---
## Ways to think about MLM

1. Multiple levels of focus (L1 vs L2, within vs between, trial vs person)
2. Creating piles of meaningful variance

3. Parameters that depend on parameters
4. Shrinkage towards mean of some population (higher N, less shrinkage)


---
### Parameters that depend on parameters

![](mlm-8_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;


---
###  Shrinkage towards mean 

.small[
`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$`

`$$\mu_i  = \beta_{0[i]}$$` 

`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu},  \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`

`$$\sigma_0 \sim {\operatorname{Normal}(0, 1.5)}$$`



`$$\sigma \sim {\operatorname{Exponential}(1)}$$`
]

Half normal can turn on/off shrinkage. Setting SD to zero says everyone is the same (complete). Infinite SD says all groups are unrelated (no pooling). In the middle gives you partial pooling. Remember, with MLE, you are using flat priors.   

---


```r
summary(mlm.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + (1 | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;
##          total post-warmup samples = 16000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.01     0.05     0.07 1.00     5205     8457
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.20 1.00     8238    10695
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     9478    11175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```



---


```r
library(tidybayes)
get_variables(mlm.1)
```

```
##   [1] "b_Intercept"         "sd_ID__Intercept"    "sigma"              
##   [4] "r_ID[6,Intercept]"   "r_ID[29,Intercept]"  "r_ID[34,Intercept]" 
##   [7] "r_ID[36,Intercept]"  "r_ID[37,Intercept]"  "r_ID[48,Intercept]" 
##  [10] "r_ID[53,Intercept]"  "r_ID[54,Intercept]"  "r_ID[58,Intercept]" 
##  [13] "r_ID[61,Intercept]"  "r_ID[66,Intercept]"  "r_ID[67,Intercept]" 
##  [16] "r_ID[69,Intercept]"  "r_ID[71,Intercept]"  "r_ID[74,Intercept]" 
##  [19] "r_ID[75,Intercept]"  "r_ID[76,Intercept]"  "r_ID[78,Intercept]" 
##  [22] "r_ID[79,Intercept]"  "r_ID[80,Intercept]"  "r_ID[81,Intercept]" 
##  [25] "r_ID[82,Intercept]"  "r_ID[85,Intercept]"  "r_ID[86,Intercept]" 
##  [28] "r_ID[87,Intercept]"  "r_ID[89,Intercept]"  "r_ID[91,Intercept]" 
##  [31] "r_ID[92,Intercept]"  "r_ID[93,Intercept]"  "r_ID[94,Intercept]" 
##  [34] "r_ID[96,Intercept]"  "r_ID[97,Intercept]"  "r_ID[98,Intercept]" 
##  [37] "r_ID[99,Intercept]"  "r_ID[101,Intercept]" "r_ID[102,Intercept]"
##  [40] "r_ID[103,Intercept]" "r_ID[104,Intercept]" "r_ID[105,Intercept]"
##  [43] "r_ID[106,Intercept]" "r_ID[110,Intercept]" "r_ID[112,Intercept]"
##  [46] "r_ID[114,Intercept]" "r_ID[115,Intercept]" "r_ID[116,Intercept]"
##  [49] "r_ID[120,Intercept]" "r_ID[122,Intercept]" "r_ID[125,Intercept]"
##  [52] "r_ID[127,Intercept]" "r_ID[129,Intercept]" "r_ID[135,Intercept]"
##  [55] "r_ID[136,Intercept]" "r_ID[137,Intercept]" "r_ID[140,Intercept]"
##  [58] "r_ID[141,Intercept]" "r_ID[142,Intercept]" "r_ID[143,Intercept]"
##  [61] "r_ID[144,Intercept]" "r_ID[146,Intercept]" "r_ID[149,Intercept]"
##  [64] "r_ID[150,Intercept]" "r_ID[152,Intercept]" "r_ID[153,Intercept]"
##  [67] "r_ID[155,Intercept]" "r_ID[156,Intercept]" "r_ID[159,Intercept]"
##  [70] "r_ID[160,Intercept]" "r_ID[162,Intercept]" "r_ID[163,Intercept]"
##  [73] "r_ID[165,Intercept]" "r_ID[167,Intercept]" "r_ID[169,Intercept]"
##  [76] "r_ID[171,Intercept]" "r_ID[174,Intercept]" "r_ID[182,Intercept]"
##  [79] "r_ID[187,Intercept]" "r_ID[189,Intercept]" "r_ID[190,Intercept]"
##  [82] "r_ID[193,Intercept]" "r_ID[194,Intercept]" "r_ID[201,Intercept]"
##  [85] "r_ID[204,Intercept]" "r_ID[205,Intercept]" "r_ID[208,Intercept]"
##  [88] "r_ID[209,Intercept]" "r_ID[211,Intercept]" "r_ID[214,Intercept]"
##  [91] "r_ID[219,Intercept]" "r_ID[222,Intercept]" "r_ID[223,Intercept]"
##  [94] "r_ID[229,Intercept]" "prior_Intercept"     "prior_sigma"        
##  [97] "prior_sd_ID"         "lp__"                "accept_stat__"      
## [100] "stepsize__"          "treedepth__"         "n_leapfrog__"       
## [103] "divergent__"         "energy__"
```


---


```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) 
```

```
## # A tibble: 1,456,000 x 6
## # Groups:   ID, term [91]
##       ID term          r_ID .chain .iteration .draw
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;
##  1     6 Intercept  0.00585      1          1     1
##  2     6 Intercept  0.00850      1          2     2
##  3     6 Intercept  0.00333      1          3     3
##  4     6 Intercept  0.00222      1          4     4
##  5     6 Intercept  0.0231       1          5     5
##  6     6 Intercept  0.0147       1          6     6
##  7     6 Intercept  0.00593      1          7     7
##  8     6 Intercept  0.00779      1          8     8
##  9     6 Intercept -0.00722      1          9     9
## 10     6 Intercept  0.00491      1         10    10
## # … with 1,455,990 more rows
```
16000 samples (4 chains * 4k iterations) * 91 people

---


```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) %&gt;%
 median_qi()
```

```
## # A tibble: 91 x 8
## # Groups:   ID [91]
##       ID term           r_ID   .lower   .upper .width .point .interval
##    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
##  1     6 Intercept  0.000352 -0.0483   0.0496    0.95 median qi       
##  2    29 Intercept -0.0479   -0.105    0.00966   0.95 median qi       
##  3    34 Intercept -0.0611   -0.118   -0.00271   0.95 median qi       
##  4    36 Intercept -0.0224   -0.0794   0.0357    0.95 median qi       
##  5    37 Intercept -0.00303  -0.0515   0.0454    0.95 median qi       
##  6    48 Intercept  0.0231   -0.0250   0.0720    0.95 median qi       
##  7    53 Intercept  0.000113 -0.0581   0.0581    0.95 median qi       
##  8    54 Intercept  0.0453   -0.00385  0.0932    0.95 median qi       
##  9    58 Intercept  0.0367   -0.0140   0.0871    0.95 median qi       
## 10    61 Intercept -0.0746   -0.131   -0.0176    0.95 median qi       
## # … with 81 more rows
```


---
.pull-left[

```r
mlm.1 %&gt;%
  spread_draws(r_ID[ID, term]) %&gt;%
  left_join(mlm %&gt;% select (ID, CON) %&gt;% group_by(ID) %&gt;%  mutate(CON = mean(CON))) %&gt;% 
 median_qi() %&gt;% 
ggplot(aes(x = ID)) +
  geom_point(aes(y = r_ID), color = "orange2") +
  geom_point(aes(y = CON), shape = 1) 
```
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
]

---


```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) 
```

```
## # A tibble: 1,456,000 x 7
## # Groups:   ID, term [91]
##    .chain .iteration .draw b_Intercept    ID term          r_ID
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1      1          1     1       0.185     6 Intercept  0.00585
##  2      1          1     1       0.185    29 Intercept -0.0423 
##  3      1          1     1       0.185    34 Intercept -0.149  
##  4      1          1     1       0.185    36 Intercept  0.0127 
##  5      1          1     1       0.185    37 Intercept  0.0299 
##  6      1          1     1       0.185    48 Intercept  0.0213 
##  7      1          1     1       0.185    53 Intercept  0.00556
##  8      1          1     1       0.185    54 Intercept  0.0314 
##  9      1          1     1       0.185    58 Intercept  0.0559 
## 10      1          1     1       0.185    61 Intercept -0.0675 
## # … with 1,455,990 more rows
```

---


```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) %&gt;% 
   mutate(person_I = b_Intercept + r_ID) 
```

```
## # A tibble: 1,456,000 x 8
## # Groups:   ID, term [91]
##    .chain .iteration .draw b_Intercept    ID term          r_ID person_I
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;
##  1      1          1     1       0.185     6 Intercept  0.00585   0.191 
##  2      1          1     1       0.185    29 Intercept -0.0423    0.143 
##  3      1          1     1       0.185    34 Intercept -0.149     0.0363
##  4      1          1     1       0.185    36 Intercept  0.0127    0.198 
##  5      1          1     1       0.185    37 Intercept  0.0299    0.215 
##  6      1          1     1       0.185    48 Intercept  0.0213    0.206 
##  7      1          1     1       0.185    53 Intercept  0.00556   0.191 
##  8      1          1     1       0.185    54 Intercept  0.0314    0.216 
##  9      1          1     1       0.185    58 Intercept  0.0559    0.241 
## 10      1          1     1       0.185    61 Intercept -0.0675    0.118 
## # … with 1,455,990 more rows
```

---

![](mlm-8_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---
.pull-left[

```r
mlm.1 %&gt;%
  spread_draws(b_Intercept, r_ID[ID, term]) %&gt;%
   median_qi(person_I = b_Intercept + r_ID) %&gt;%
  ggplot(aes( y = reorder(ID, person_I), x = person_I, xmin = .lower, xmax = .upper)) +
  geom_pointinterval() + xlab("est") + ylab("ID")
```
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]


---
## level 1 predictors

`$$\text{y}_{ij}  \sim \operatorname{Normal}(\mu_i, \sigma_i)$$`
`$$\mu_i  = \beta_{0[i]} + \beta_{1[i]}$$` 

`$$\beta_{0[j]} \sim \operatorname{Normal}({\bar \mu}, \sigma_0)$$`
`$${\bar \mu}  \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma_0 \sim {\operatorname{Exponential}(1)}$$`
`$$\beta_{1[j]} \sim {\operatorname{Normal}(0, 1.5)}$$`
`$$\sigma \sim {\operatorname{Exponential}(1)}$$`

---


```r
get_prior(CON ~ 1 + time + (1|ID), data = mlm)
```

```
##                    prior     class      coef group resp dpar nlpar bound
## 1                                b                                      
## 2                                b      time                            
## 3 student_t(3, 0.2, 2.5) Intercept                                      
## 4   student_t(3, 0, 2.5)        sd                                      
## 5                               sd              ID                      
## 6                               sd Intercept    ID                      
## 7   student_t(3, 0, 2.5)     sigma
```


---

```r
mlm.3 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + time + (1 | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(normal(0, 1.5), class = sd), 
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      file = "mlm.3",
      data = mlm)
```


---


```r
summary(mlm.3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + time + (1 | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.01     0.05     0.07 1.00     3881     6287
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.21 1.00     6315     8219
## time         -0.00      0.00    -0.01     0.00 1.00    14445     9701
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     7774     9010
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
posterior_summary(mlm.3)
```

```
##                          Estimate    Est.Error          Q2.5         Q97.5
## b_Intercept          1.942546e-01  0.007896868  1.788568e-01  2.098866e-01
## b_time              -2.681927e-03  0.002193669 -7.005901e-03  1.673801e-03
## sd_ID__Intercept     5.658460e-02  0.005703638  4.626523e-02  6.851444e-02
## sigma                4.736330e-02  0.002946050  4.203858e-02  5.362970e-02
## r_ID[6,Intercept]    7.018953e-03  0.025941421 -4.477088e-02  5.766420e-02
## r_ID[29,Intercept]  -4.288168e-02  0.030110115 -1.010812e-01  1.694325e-02
## r_ID[34,Intercept]  -5.532291e-02  0.029653091 -1.141263e-01  2.582092e-03
## r_ID[36,Intercept]  -2.073992e-02  0.029312647 -7.758616e-02  3.623226e-02
## r_ID[37,Intercept]   2.927101e-03  0.025757301 -4.748359e-02  5.385893e-02
## r_ID[48,Intercept]   2.927374e-02  0.025454433 -2.069050e-02  7.967467e-02
## r_ID[53,Intercept]   5.975410e-03  0.029615044 -5.215262e-02  6.417561e-02
## r_ID[54,Intercept]   4.987981e-02  0.025357223  3.141462e-04  9.938717e-02
## r_ID[58,Intercept]   3.781205e-02  0.025449617 -1.329860e-02  8.725924e-02
## r_ID[61,Intercept]  -6.960958e-02  0.029601512 -1.287767e-01 -1.212965e-02
## r_ID[66,Intercept]   4.220046e-02  0.025770815 -7.324374e-03  9.239867e-02
## r_ID[67,Intercept]   1.219182e-02  0.022834285 -3.208344e-02  5.731745e-02
## r_ID[69,Intercept]   1.209571e-02  0.028981638 -4.486022e-02  6.887835e-02
## r_ID[71,Intercept]  -1.049422e-01  0.025884046 -1.557768e-01 -5.383888e-02
## r_ID[74,Intercept]   1.382781e-02  0.025137712 -3.492902e-02  6.313762e-02
## r_ID[75,Intercept]   5.936441e-02  0.029822870  5.979624e-04  1.167617e-01
## r_ID[76,Intercept]  -7.309928e-02  0.025467067 -1.222700e-01 -2.214936e-02
## r_ID[78,Intercept]   4.705561e-02  0.025342553 -3.398375e-03  9.765876e-02
## r_ID[79,Intercept]  -4.334924e-02  0.029853403 -1.015261e-01  1.540862e-02
## r_ID[80,Intercept]  -3.339505e-02  0.029687291 -9.119510e-02  2.508338e-02
## r_ID[81,Intercept]   4.447478e-02  0.025601175 -5.385413e-03  9.495197e-02
## r_ID[82,Intercept]   1.195476e-01  0.023273396  7.425035e-02  1.650440e-01
## r_ID[85,Intercept]  -2.690532e-02  0.028646717 -8.389778e-02  2.963529e-02
## r_ID[86,Intercept]  -5.676805e-02  0.029380950 -1.144974e-01  2.941393e-04
## r_ID[87,Intercept]  -5.928211e-02  0.029164950 -1.159989e-01 -2.287059e-03
## r_ID[89,Intercept]  -4.527398e-02  0.025330966 -9.450856e-02  4.540375e-03
## r_ID[91,Intercept]  -1.532585e-02  0.025444134 -6.638986e-02  3.385095e-02
## r_ID[92,Intercept]   9.491611e-03  0.025219481 -4.091591e-02  5.805656e-02
## r_ID[93,Intercept]   5.077455e-02  0.025354322  1.193554e-03  1.008230e-01
## r_ID[94,Intercept]  -5.892650e-02  0.025191670 -1.086142e-01 -9.250394e-03
## r_ID[96,Intercept]   2.835245e-02  0.029943507 -3.082319e-02  8.683453e-02
## r_ID[97,Intercept]   4.918515e-02  0.022387705  4.716593e-03  9.271133e-02
## r_ID[98,Intercept]  -4.323536e-02  0.022129181 -8.634748e-02  1.932214e-04
## r_ID[99,Intercept]  -6.992259e-03  0.022367462 -5.073480e-02  3.707988e-02
## r_ID[101,Intercept]  4.368875e-02  0.025231051 -5.657602e-03  9.236464e-02
## r_ID[102,Intercept] -2.286680e-03  0.022617700 -4.701696e-02  4.129274e-02
## r_ID[103,Intercept] -2.858660e-02  0.029047084 -8.555045e-02  2.806853e-02
## r_ID[104,Intercept] -1.400494e-02  0.029224754 -7.226492e-02  4.344003e-02
## r_ID[105,Intercept] -2.341769e-02  0.025371371 -7.358910e-02  2.630763e-02
## r_ID[106,Intercept]  5.518715e-03  0.025254933 -4.385803e-02  5.516233e-02
## r_ID[110,Intercept] -6.420876e-03  0.025449730 -5.551179e-02  4.357768e-02
## r_ID[112,Intercept] -7.828839e-02  0.025776244 -1.286628e-01 -2.710820e-02
## r_ID[114,Intercept] -4.690738e-02  0.025415156 -9.624883e-02  3.264763e-03
## r_ID[115,Intercept] -4.035656e-02  0.025606948 -9.035083e-02  1.005650e-02
## r_ID[116,Intercept] -3.764640e-02  0.025039753 -8.637578e-02  1.144856e-02
## r_ID[120,Intercept]  1.176644e-01  0.030645188  5.744574e-02  1.784991e-01
## r_ID[122,Intercept] -1.657607e-02  0.025368556 -6.633496e-02  3.321338e-02
## r_ID[125,Intercept]  3.729669e-03  0.025394199 -4.627640e-02  5.315664e-02
## r_ID[127,Intercept]  2.882648e-02  0.025672193 -2.123581e-02  7.919260e-02
## r_ID[129,Intercept] -4.629830e-02  0.029336919 -1.037252e-01  1.110150e-02
## r_ID[135,Intercept]  8.621197e-02  0.025226545  3.705872e-02  1.357631e-01
## r_ID[136,Intercept]  8.127130e-02  0.025404208  3.028722e-02  1.308663e-01
## r_ID[137,Intercept]  7.373746e-02  0.029807884  1.539079e-02  1.314328e-01
## r_ID[140,Intercept] -4.841797e-02  0.030039326 -1.074086e-01  1.015885e-02
## r_ID[141,Intercept]  6.833951e-04  0.029315927 -5.671917e-02  5.779745e-02
## r_ID[142,Intercept] -9.292602e-03  0.028636919 -6.446483e-02  4.718955e-02
## r_ID[143,Intercept]  8.714624e-02  0.029833395  2.884853e-02  1.452437e-01
## r_ID[144,Intercept] -1.256967e-02  0.029670900 -7.043836e-02  4.465387e-02
## r_ID[146,Intercept]  1.429103e-02  0.029313546 -4.287515e-02  7.172247e-02
## r_ID[149,Intercept]  2.744938e-02  0.029398816 -3.018099e-02  8.416187e-02
## r_ID[150,Intercept]  7.730344e-03  0.028973000 -4.894491e-02  6.401379e-02
## r_ID[152,Intercept]  2.213916e-02  0.029119791 -3.451249e-02  7.935258e-02
## r_ID[153,Intercept] -3.225273e-02  0.029160136 -8.975912e-02  2.448037e-02
## r_ID[155,Intercept] -3.506993e-02  0.029068786 -9.176123e-02  2.146021e-02
## r_ID[156,Intercept] -4.429070e-02  0.024890695 -9.325590e-02  4.405192e-03
## r_ID[159,Intercept] -5.080707e-02  0.028977539 -1.084377e-01  5.960883e-03
## r_ID[160,Intercept] -1.060459e-02  0.029401585 -6.848742e-02  4.595146e-02
## r_ID[162,Intercept]  1.633366e-01  0.026898546  1.098817e-01  2.149524e-01
## r_ID[163,Intercept] -4.263144e-02  0.029107364 -9.919982e-02  1.353090e-02
## r_ID[165,Intercept]  4.155547e-03  0.029313443 -5.360351e-02  6.138816e-02
## r_ID[167,Intercept] -4.269228e-02  0.029376594 -1.003594e-01  1.481150e-02
## r_ID[169,Intercept]  3.982445e-02  0.029145165 -1.793443e-02  9.760079e-02
## r_ID[171,Intercept]  2.772443e-02  0.029354252 -3.058629e-02  8.515212e-02
## r_ID[174,Intercept]  5.357064e-02  0.028875050 -3.073679e-03  1.097794e-01
## r_ID[182,Intercept] -2.748676e-03  0.029452729 -5.918770e-02  5.562626e-02
## r_ID[187,Intercept] -9.302807e-02  0.029802553 -1.516136e-01 -3.479083e-02
## r_ID[189,Intercept]  2.287741e-02  0.029206971 -3.421937e-02  8.084216e-02
## r_ID[190,Intercept] -4.744368e-02  0.029198595 -1.039006e-01  1.080793e-02
## r_ID[193,Intercept]  1.133945e-02  0.028440137 -4.527217e-02  6.749816e-02
## r_ID[194,Intercept] -5.154635e-02  0.029726500 -1.088040e-01  7.064636e-03
## r_ID[201,Intercept] -2.591737e-03  0.028926720 -5.903368e-02  5.350875e-02
## r_ID[204,Intercept]  7.098459e-02  0.030249099  1.181447e-02  1.305028e-01
## r_ID[205,Intercept]  1.769111e-03  0.028782975 -5.576374e-02  5.771985e-02
## r_ID[208,Intercept]  1.049846e-02  0.029495558 -4.723132e-02  6.880338e-02
## r_ID[209,Intercept]  1.659315e-02  0.028719383 -3.889772e-02  7.294587e-02
## r_ID[211,Intercept] -3.766254e-02  0.028913961 -9.489054e-02  1.918218e-02
## r_ID[214,Intercept]  6.297490e-02  0.029148063  5.743129e-03  1.205611e-01
## r_ID[219,Intercept]  3.050963e-02  0.029443432 -2.716614e-02  8.776401e-02
## r_ID[222,Intercept] -3.964643e-02  0.029367107 -9.815356e-02  1.754864e-02
## r_ID[223,Intercept] -1.750833e-02  0.029072629 -7.544184e-02  3.974895e-02
## r_ID[229,Intercept] -3.663063e-02  0.029287956 -9.366597e-02  2.057921e-02
## prior_Intercept     -1.685822e-02  1.478493730 -2.955803e+00  2.896009e+00
## prior_b             -1.742017e-03  1.506820774 -2.933896e+00  2.927901e+00
## prior_sigma          9.982588e-01  1.008688946  2.677746e-02  3.757328e+00
## prior_sd_ID          1.187039e+00  0.898901972  4.875643e-02  3.358880e+00
## lp__                 2.300588e+02 11.380311991  2.065194e+02  2.513384e+02
```

---


```r
pp_check(mlm.3)
```

```
## Using 10 posterior samples for ppc type 'dens_overlay' by default.
```

![](mlm-8_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;


---


```r
plot(mlm.3)
```

![](mlm-8_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;

---
## Mr. P
aka multilevel regression with poststratification

One of the biggest problems with psych data is that it is unrepresentative. Often survey weights are used. But if we know the  distribution of the broader population, we can reweight (post-stratify) our results to get more accurate estimates. Instead of making assumptions about how the observed sample was produced from the population, we make assumptions about how the observed sample can be used to reconstruct the rest of the population

https://www.monicaalexander.com/posts/2019-08-07-mrp/
https://bookdown.org/content/4857/models-with-memory.html#summary-bonus-post-stratification-in-an-example


---

```r
mrp &lt;-load("mrp.rds")
mrp
```

```
## [1] "d"           "cell_counts"
```


```r
head(d)
```

```
## # A tibble: 6 x 5
##   kept_name state_name     age_group decade_married educ_group
##       &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;     
## 1         0 ohio           50        1979           &gt;BA       
## 2         0 virginia       35        1999           &gt;BA       
## 3         1 new york       35        2009           &gt;BA       
## 4         0 rhode island   55        1999           &gt;BA       
## 5         0 illinois       35        2009           &gt;BA       
## 6         0 north carolina 25        2009           &gt;BA
```


```r
head(cell_counts)
```

```
## # A tibble: 6 x 5
##   state_name age_group decade_married educ_group     n
##   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;
## 1 alabama    25        1999           &lt;BA        19012
## 2 alabama    25        2009           &lt;BA        37488
## 3 alabama    25        1999           &gt;BA          959
## 4 alabama    25        2009           &gt;BA         5319
## 5 alabama    25        1999           BA          2986
## 6 alabama    25        2009           BA         14261
```

---



```r
mlm.mrp &lt;-
  brm(family = binomial,
      kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name),
      prior = c(prior(normal(-1, 1), class = Intercept),
                prior(exponential(1), class = sd)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .98),
      data = d,
      file = "mlm.mrp")
```


---

![](mlm-8_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;


---


```r
age_prop &lt;- 
  cell_counts %&gt;% 
  group_by(age_group) %&gt;% 
  mutate(prop = n / sum(n)) %&gt;% 
  ungroup()

age_prop
```

```
## # A tibble: 6,058 x 6
##    state_name age_group decade_married educ_group     n      prop
##    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;
##  1 alabama    25        1999           &lt;BA        19012 0.00414  
##  2 alabama    25        2009           &lt;BA        37488 0.00816  
##  3 alabama    25        1999           &gt;BA          959 0.000209 
##  4 alabama    25        2009           &gt;BA         5319 0.00116  
##  5 alabama    25        1999           BA          2986 0.000650 
##  6 alabama    25        2009           BA         14261 0.00310  
##  7 alaska     25        1999           &lt;BA         3320 0.000723 
##  8 alaska     25        2009           &lt;BA         7001 0.00152  
##  9 alaska     25        1999           &gt;BA          159 0.0000346
## 10 alaska     25        2009           &gt;BA          435 0.0000947
## # … with 6,048 more rows
```


---
.small[

```r
p &lt;- 
  add_predicted_draws(mlm.mrp, newdata = age_prop %&gt;% 
                        filter(age_group &gt; 20, 
                               age_group &lt; 80, 
                               decade_married &gt; 1969),
                      allow_new_levels = T)
p
```

```
## # A tibble: 24,232,000 x 11
## # Groups:   state_name, age_group, decade_married, educ_group, n, prop, .row
## #   [6,058]
##    state_name age_group decade_married educ_group     n    prop  .row .chain
##    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;
##  1 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  2 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  3 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  4 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  5 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  6 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  7 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  8 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
##  9 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
## 10 alabama    25        1999           &lt;BA        19012 0.00414     1     NA
## # … with 24,231,990 more rows, and 3 more variables: .iteration &lt;int&gt;,
## #   .draw &lt;int&gt;, .prediction &lt;int&gt;
```
]

6,058 census categories * 4,000 samples = 24,232,000

---

If we group the results by age_group and .draw, we can sum the product of the posterior predictions and the weights, which will leave us with 4,000 stratified posterior draws for each of the 11 levels of age_group

`$$\frac{\sum_i N_i p_i}{\sum_i N_i}$$`

---


```r
p &lt;-
  p %&gt;% 
  group_by(age_group, .draw) %&gt;% 
  summarise(kept_name_predict = sum(.prediction * prop)) %&gt;% 
  group_by(age_group) %&gt;% 
  mean_qi(kept_name_predict)
```

```
## `summarise()` regrouping output by 'age_group' (override with `.groups` argument)
```

```r
p
```

```
## # A tibble: 11 x 7
##    age_group kept_name_predict .lower .upper .width .point .interval
##    &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
##  1 25                    0.175 0.0923  0.277   0.95 mean   qi       
##  2 30                    0.181 0.112   0.261   0.95 mean   qi       
##  3 35                    0.217 0.146   0.298   0.95 mean   qi       
##  4 40                    0.245 0.171   0.328   0.95 mean   qi       
##  5 45                    0.279 0.201   0.370   0.95 mean   qi       
##  6 50                    0.300 0.218   0.392   0.95 mean   qi       
##  7 55                    0.323 0.233   0.425   0.95 mean   qi       
##  8 60                    0.437 0.332   0.546   0.95 mean   qi       
##  9 65                    0.561 0.413   0.702   0.95 mean   qi       
## 10 70                    0.515 0.292   0.755   0.95 mean   qi       
## 11 75                    0.227 0.0452  0.511   0.95 mean   qi
```



---
.pull-left[
![](mlm-8_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;
]

.pull-right[The survey has an over-sample of highly educated women, who are more likely to keep their name, hence the regularization of MLM]

---
### Not limited to surveys

Example (using brms) with experimental data -- can you generalize from your sample of undergraduates to other undergraduates at your university, let alone undergraduates in general, let alone young adults, to say nothing about the population of humans. 
https://arxiv.org/pdf/1906.11323.pdf


---
## Random slopes

.pull-left[
1. Modeling the joint population of intercepts and slopes, which means by modeling their covariance. We will use joint multivariate Gaussian distribution b/c it is max entropy distribution

2. Involves lots of interactions
]
.pull-right[
Level 1:
`$${Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij}$$`
Level 2: 
`$${\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}$$`  
`$${\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j}$$`

`$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} &amp; \tau_{01}\\ 
  0,  \tau_{01} &amp; \tau_{10}^{2}
\end{pmatrix}$$`



]

---
## Random slopes

.pull-left[.tiny[
`$$y_i \sim \text{Normal}(\mu, \sigma)$$` 
`$$\mu_i = \beta_{0j} + \beta_{1j}X_{ij}$$`
`$$(\beta_{0j}, \beta_{1j}) \sim \text{MVNormal}  ([\beta_0, \beta_1], \Sigma)$$`


`$$\Sigma = 
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;0\\
0&amp;\sigma_{\beta_1}
\end{array}\right)R
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;0\\
0&amp;\sigma_{\beta_1}
\end{array}\right)$$`
`$$\beta_0 \sim \text{Normal}(0, 1)$$`

`$$\beta_1 \sim \text{Normal}(0, 1)$$`
`$$\sigma_{\beta_0} \sim \text{Exponential}(1)$$`
`$$\sigma_{\beta_1} \sim \text{Exponential}(1)$$`
`$$\sigma \sim \text{Exponential}(1)$$`
`$$R \sim \text{LKJcorr(2)}$$`

]]

.pull-right[
where Σ is the covariance matrix 
`$$\Sigma = 
\left(\begin{array}{cc}
\sigma^2_{\beta_0}&amp;\sigma_{\beta_0}\sigma_{\beta_1}\rho\\
\sigma_{\beta_0}\sigma_{\beta_1}\rho&amp;\sigma^2_{\beta_1}
\end{array}\right)$$`

and R is the correlation matrix R = `\(\begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{bmatrix}\)`.

With more random effects, this matrix expands.



]


---
## lkj
.pull-left[Our regularizing prior for correlation matrices. Has one parameter to tune the potential associations. 
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-39-1.png)&lt;!-- --&gt;

]


---


```r
get_prior(family = gaussian,
      CON ~ 1 + time + (1 + time | ID),
      data = mlm)
```

```
##                     prior     class      coef group resp dpar nlpar bound
## 1                                 b                                      
## 2                                 b      time                            
## 3                  lkj(1)       cor                                      
## 4                               cor              ID                      
## 5  student_t(3, 0.2, 2.5) Intercept                                      
## 6    student_t(3, 0, 2.5)        sd                                      
## 7                                sd              ID                      
## 8                                sd Intercept    ID                      
## 9                                sd      time    ID                      
## 10   student_t(3, 0, 2.5)     sigma
```

6 parameters we are estimating

---


```r
mlm.4 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + time + (1 + time | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(normal(0, 1.5), class = sd, coef = Intercept, group = ID), 
                prior(normal(0, 1.5), class = sd, coef = time, group = ID), 
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      file = "mlm.4",
      data = mlm)
```


---


```r
summary(mlm.4)
```

```
## Warning: There were 61 divergent transitions after warmup. Increasing
## adapt_delta above 0.8 may help. See http://mc-stan.org/misc/
## warnings.html#divergent-transitions-after-warmup
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + time + (1 + time | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.06      0.01     0.05     0.07 1.00     3790     5168
## sd(time)                0.00      0.00     0.00     0.01 1.00     1086     1611
## cor(Intercept,time)    -0.09      0.39    -0.77     0.71 1.00     8695     6912
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.21 1.00     4194     6987
## time         -0.00      0.00    -0.01     0.00 1.00    11407     9656
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     2513     2163
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

![](mlm-8_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

---


```
## `summarise()` regrouping output by 'ID' (override with `.groups` argument)
```

![](mlm-8_files/figure-html/unnamed-chunk-44-1.png)&lt;!-- --&gt;


---


```r
get_variables(mlm.4)
```

```
##   [1] "b_Intercept"             "b_time"                 
##   [3] "sd_ID__Intercept"        "sd_ID__time"            
##   [5] "cor_ID__Intercept__time" "sigma"                  
##   [7] "r_ID[6,Intercept]"       "r_ID[29,Intercept]"     
##   [9] "r_ID[34,Intercept]"      "r_ID[36,Intercept]"     
##  [11] "r_ID[37,Intercept]"      "r_ID[48,Intercept]"     
##  [13] "r_ID[53,Intercept]"      "r_ID[54,Intercept]"     
##  [15] "r_ID[58,Intercept]"      "r_ID[61,Intercept]"     
##  [17] "r_ID[66,Intercept]"      "r_ID[67,Intercept]"     
##  [19] "r_ID[69,Intercept]"      "r_ID[71,Intercept]"     
##  [21] "r_ID[74,Intercept]"      "r_ID[75,Intercept]"     
##  [23] "r_ID[76,Intercept]"      "r_ID[78,Intercept]"     
##  [25] "r_ID[79,Intercept]"      "r_ID[80,Intercept]"     
##  [27] "r_ID[81,Intercept]"      "r_ID[82,Intercept]"     
##  [29] "r_ID[85,Intercept]"      "r_ID[86,Intercept]"     
##  [31] "r_ID[87,Intercept]"      "r_ID[89,Intercept]"     
##  [33] "r_ID[91,Intercept]"      "r_ID[92,Intercept]"     
##  [35] "r_ID[93,Intercept]"      "r_ID[94,Intercept]"     
##  [37] "r_ID[96,Intercept]"      "r_ID[97,Intercept]"     
##  [39] "r_ID[98,Intercept]"      "r_ID[99,Intercept]"     
##  [41] "r_ID[101,Intercept]"     "r_ID[102,Intercept]"    
##  [43] "r_ID[103,Intercept]"     "r_ID[104,Intercept]"    
##  [45] "r_ID[105,Intercept]"     "r_ID[106,Intercept]"    
##  [47] "r_ID[110,Intercept]"     "r_ID[112,Intercept]"    
##  [49] "r_ID[114,Intercept]"     "r_ID[115,Intercept]"    
##  [51] "r_ID[116,Intercept]"     "r_ID[120,Intercept]"    
##  [53] "r_ID[122,Intercept]"     "r_ID[125,Intercept]"    
##  [55] "r_ID[127,Intercept]"     "r_ID[129,Intercept]"    
##  [57] "r_ID[135,Intercept]"     "r_ID[136,Intercept]"    
##  [59] "r_ID[137,Intercept]"     "r_ID[140,Intercept]"    
##  [61] "r_ID[141,Intercept]"     "r_ID[142,Intercept]"    
##  [63] "r_ID[143,Intercept]"     "r_ID[144,Intercept]"    
##  [65] "r_ID[146,Intercept]"     "r_ID[149,Intercept]"    
##  [67] "r_ID[150,Intercept]"     "r_ID[152,Intercept]"    
##  [69] "r_ID[153,Intercept]"     "r_ID[155,Intercept]"    
##  [71] "r_ID[156,Intercept]"     "r_ID[159,Intercept]"    
##  [73] "r_ID[160,Intercept]"     "r_ID[162,Intercept]"    
##  [75] "r_ID[163,Intercept]"     "r_ID[165,Intercept]"    
##  [77] "r_ID[167,Intercept]"     "r_ID[169,Intercept]"    
##  [79] "r_ID[171,Intercept]"     "r_ID[174,Intercept]"    
##  [81] "r_ID[182,Intercept]"     "r_ID[187,Intercept]"    
##  [83] "r_ID[189,Intercept]"     "r_ID[190,Intercept]"    
##  [85] "r_ID[193,Intercept]"     "r_ID[194,Intercept]"    
##  [87] "r_ID[201,Intercept]"     "r_ID[204,Intercept]"    
##  [89] "r_ID[205,Intercept]"     "r_ID[208,Intercept]"    
##  [91] "r_ID[209,Intercept]"     "r_ID[211,Intercept]"    
##  [93] "r_ID[214,Intercept]"     "r_ID[219,Intercept]"    
##  [95] "r_ID[222,Intercept]"     "r_ID[223,Intercept]"    
##  [97] "r_ID[229,Intercept]"     "r_ID[6,time]"           
##  [99] "r_ID[29,time]"           "r_ID[34,time]"          
## [101] "r_ID[36,time]"           "r_ID[37,time]"          
## [103] "r_ID[48,time]"           "r_ID[53,time]"          
## [105] "r_ID[54,time]"           "r_ID[58,time]"          
## [107] "r_ID[61,time]"           "r_ID[66,time]"          
## [109] "r_ID[67,time]"           "r_ID[69,time]"          
## [111] "r_ID[71,time]"           "r_ID[74,time]"          
## [113] "r_ID[75,time]"           "r_ID[76,time]"          
## [115] "r_ID[78,time]"           "r_ID[79,time]"          
## [117] "r_ID[80,time]"           "r_ID[81,time]"          
## [119] "r_ID[82,time]"           "r_ID[85,time]"          
## [121] "r_ID[86,time]"           "r_ID[87,time]"          
## [123] "r_ID[89,time]"           "r_ID[91,time]"          
## [125] "r_ID[92,time]"           "r_ID[93,time]"          
## [127] "r_ID[94,time]"           "r_ID[96,time]"          
## [129] "r_ID[97,time]"           "r_ID[98,time]"          
## [131] "r_ID[99,time]"           "r_ID[101,time]"         
## [133] "r_ID[102,time]"          "r_ID[103,time]"         
## [135] "r_ID[104,time]"          "r_ID[105,time]"         
## [137] "r_ID[106,time]"          "r_ID[110,time]"         
## [139] "r_ID[112,time]"          "r_ID[114,time]"         
## [141] "r_ID[115,time]"          "r_ID[116,time]"         
## [143] "r_ID[120,time]"          "r_ID[122,time]"         
## [145] "r_ID[125,time]"          "r_ID[127,time]"         
## [147] "r_ID[129,time]"          "r_ID[135,time]"         
## [149] "r_ID[136,time]"          "r_ID[137,time]"         
## [151] "r_ID[140,time]"          "r_ID[141,time]"         
## [153] "r_ID[142,time]"          "r_ID[143,time]"         
## [155] "r_ID[144,time]"          "r_ID[146,time]"         
## [157] "r_ID[149,time]"          "r_ID[150,time]"         
## [159] "r_ID[152,time]"          "r_ID[153,time]"         
## [161] "r_ID[155,time]"          "r_ID[156,time]"         
## [163] "r_ID[159,time]"          "r_ID[160,time]"         
## [165] "r_ID[162,time]"          "r_ID[163,time]"         
## [167] "r_ID[165,time]"          "r_ID[167,time]"         
## [169] "r_ID[169,time]"          "r_ID[171,time]"         
## [171] "r_ID[174,time]"          "r_ID[182,time]"         
## [173] "r_ID[187,time]"          "r_ID[189,time]"         
## [175] "r_ID[190,time]"          "r_ID[193,time]"         
## [177] "r_ID[194,time]"          "r_ID[201,time]"         
## [179] "r_ID[204,time]"          "r_ID[205,time]"         
## [181] "r_ID[208,time]"          "r_ID[209,time]"         
## [183] "r_ID[211,time]"          "r_ID[214,time]"         
## [185] "r_ID[219,time]"          "r_ID[222,time]"         
## [187] "r_ID[223,time]"          "r_ID[229,time]"         
## [189] "lp__"                    "accept_stat__"          
## [191] "stepsize__"              "treedepth__"            
## [193] "n_leapfrog__"            "divergent__"            
## [195] "energy__"
```

---


```r
mlm.4 %&gt;%
  spread_draws(r_ID[ID,term]) 
```

```
## # A tibble: 2,184,000 x 6
## # Groups:   ID, term [182]
##       ID term          r_ID .chain .iteration .draw
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;
##  1     6 Intercept  0.0167       1          1     1
##  2     6 Intercept -0.00230      1          2     2
##  3     6 Intercept -0.00393      1          3     3
##  4     6 Intercept  0.0300       1          4     4
##  5     6 Intercept  0.0618       1          5     5
##  6     6 Intercept  0.0212       1          6     6
##  7     6 Intercept  0.0216       1          7     7
##  8     6 Intercept  0.0324       1          8     8
##  9     6 Intercept  0.00742      1          9     9
## 10     6 Intercept -0.00277      1         10    10
## # … with 2,183,990 more rows
```

---


```r
mlm.4 %&gt;%
  spread_draws(b_time, r_ID[ID, term]) %&gt;%
  filter(term == "time") %&gt;% 
  mutate(person_t = b_time + r_ID) %&gt;% 
 median_qi()
```

```
## # A tibble: 91 x 14
## # Groups:   ID [91]
##       ID term    b_time b_time.lower b_time.upper     r_ID r_ID.lower r_ID.upper
##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1     6 time  -0.00285     -0.00731      0.00144  2.72e-5   -0.0100     0.0105 
##  2    29 time  -0.00285     -0.00731      0.00144 -3.63e-4   -0.0134     0.00852
##  3    34 time  -0.00285     -0.00731      0.00144 -6.41e-4   -0.0149     0.00700
##  4    36 time  -0.00285     -0.00731      0.00144  1.08e-4   -0.00970    0.0123 
##  5    37 time  -0.00285     -0.00731      0.00144 -2.01e-4   -0.0120     0.00784
##  6    48 time  -0.00285     -0.00731      0.00144 -3.71e-5   -0.0113     0.00914
##  7    53 time  -0.00285     -0.00731      0.00144 -2.63e-4   -0.0134     0.00760
##  8    54 time  -0.00285     -0.00731      0.00144  2.54e-4   -0.00834    0.0119 
##  9    58 time  -0.00285     -0.00731      0.00144  5.47e-4   -0.00667    0.0158 
## 10    61 time  -0.00285     -0.00731      0.00144 -8.65e-5   -0.0104     0.0103 
## # … with 81 more rows, and 6 more variables: person_t &lt;dbl&gt;,
## #   person_t.lower &lt;dbl&gt;, person_t.upper &lt;dbl&gt;, .width &lt;dbl&gt;, .point &lt;chr&gt;,
## #   .interval &lt;chr&gt;
```

---
.pull-left[

```r
mlm.4 %&gt;%
  spread_draws(b_time, r_ID[ID, term]) %&gt;%
  filter(term == "time") %&gt;% 
  median_qi(person_t = b_time + r_ID) %&gt;%
  ggplot(aes( y = reorder(ID, person_t), x = person_t, xmin = .lower, xmax = .upper)) +
  geom_pointinterval() + xlab("est") + ylab("ID")
```
]

.pull-right[
![](mlm-8_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;
]

---

coef = fixef + raneff

```r
ranef(mlm.4)
```

```
## $ID
## , , Intercept
## 
##          Estimate  Est.Error         Q2.5         Q97.5
## 6    0.0076575283 0.03261206 -0.055285947  0.0734524545
## 29  -0.0381945767 0.03473018 -0.105735225  0.0314158619
## 34  -0.0475070406 0.03639438 -0.115465540  0.0285581743
## 36  -0.0219513345 0.03058589 -0.083114064  0.0370777546
## 37   0.0071953645 0.03082998 -0.051138524  0.0710623633
## 48   0.0318523269 0.03162916 -0.028480413  0.0965211583
## 53   0.0102477192 0.03483843 -0.055614133  0.0819312991
## 54   0.0475687639 0.03016746 -0.011788541  0.1062336070
## 58   0.0345709800 0.02741943 -0.020712545  0.0873308903
## 61  -0.0679172242 0.03424204 -0.137830511 -0.0015997663
## 66   0.0359023496 0.02822220 -0.020350063  0.0904985701
## 67   0.0128761579 0.02530886 -0.037237491  0.0621014349
## 69   0.0133571195 0.02997804 -0.045620366  0.0724599932
## 71  -0.1032578645 0.02785669 -0.157410729 -0.0499361985
## 74   0.0134609133 0.02645707 -0.038822298  0.0650399184
## 75   0.0576076232 0.03042341 -0.002808232  0.1168981993
## 76  -0.0726999270 0.02672144 -0.125279908 -0.0202721786
## 78   0.0433264630 0.02756111 -0.011679305  0.0962292954
## 79  -0.0445169410 0.03077394 -0.105579347  0.0155725034
## 80  -0.0314162425 0.03308158 -0.096324007  0.0338404579
## 81   0.0406463063 0.02803944 -0.015485283  0.0946201426
## 82   0.1156515592 0.02624558  0.062835127  0.1660965571
## 85  -0.0271908664 0.02939996 -0.084670577  0.0302233818
## 86  -0.0574961361 0.02949805 -0.115773223  0.0004559456
## 87  -0.0580428403 0.02959244 -0.116633082  0.0007905623
## 89  -0.0462606474 0.02625648 -0.098271659  0.0055673025
## 91  -0.0150427809 0.02539744 -0.064648020  0.0356595377
## 92   0.0100921027 0.02622036 -0.040607764  0.0616153533
## 93   0.0530059925 0.02614508  0.001606915  0.1044756302
## 94  -0.0584456205 0.02568420 -0.108651203 -0.0078905785
## 96   0.0280806756 0.02949443 -0.030581180  0.0852051836
## 97   0.0488054233 0.02390221  0.001780184  0.0949028427
## 98  -0.0400190006 0.02376823 -0.086098166  0.0068833607
## 99  -0.0067950444 0.02359206 -0.052054063  0.0402333682
## 101  0.0436270589 0.02644453 -0.008092954  0.0969884415
## 102 -0.0014572816 0.02335402 -0.047238299  0.0444555791
## 103 -0.0284326790 0.02926487 -0.085342456  0.0297721160
## 104 -0.0139269675 0.02987155 -0.072649974  0.0446114387
## 105 -0.0223611444 0.02532273 -0.071808465  0.0273535945
## 106  0.0058636562 0.02596926 -0.045371288  0.0559648368
## 110 -0.0037603002 0.02704162 -0.056042030  0.0496176697
## 112 -0.0803138424 0.02694355 -0.133490053 -0.0274662976
## 114 -0.0475160181 0.02592726 -0.098046649  0.0034644994
## 115 -0.0425056843 0.02606206 -0.094204899  0.0086981471
## 116 -0.0402351231 0.02660050 -0.092964111  0.0112687658
## 120  0.1193012724 0.03087149  0.059039434  0.1792022402
## 122 -0.0175666456 0.02589030 -0.067557096  0.0330484953
## 125  0.0037335808 0.02616036 -0.047589843  0.0562678223
## 127  0.0301061118 0.02631825 -0.020912272  0.0818876123
## 129 -0.0468213180 0.02969285 -0.105521191  0.0121898338
## 135  0.0845321051 0.02678250  0.031269749  0.1365424053
## 136  0.0832642772 0.02678711  0.031036777  0.1363488239
## 137  0.0749934628 0.02958046  0.016502021  0.1329719048
## 140 -0.0485226589 0.02929405 -0.106386205  0.0091440622
## 141 -0.0006292931 0.02960949 -0.058241413  0.0572321676
## 142 -0.0092291803 0.02969485 -0.067397480  0.0495422174
## 143  0.0879464995 0.03000737  0.029979481  0.1468878509
## 144 -0.0140630266 0.02977791 -0.073843097  0.0430681218
## 146  0.0138533902 0.02974544 -0.045053055  0.0713989632
## 149  0.0295088091 0.03084905 -0.029923703  0.0917015683
## 150  0.0075048069 0.02968128 -0.051099635  0.0662643354
## 152  0.0227916641 0.02940736 -0.034893909  0.0805111002
## 153 -0.0303151493 0.02959009 -0.087325498  0.0282369991
## 155 -0.0356797953 0.03076670 -0.096700469  0.0239642754
## 156 -0.0424586126 0.02591137 -0.092057828  0.0091236407
## 159 -0.0512137281 0.02928018 -0.109261608  0.0065453112
## 160 -0.0132049674 0.03028806 -0.072737751  0.0450603370
## 162  0.1678009041 0.02881042  0.112493562  0.2257493480
## 163 -0.0425147638 0.03013531 -0.101820082  0.0162922303
## 165  0.0057929170 0.02986932 -0.053182448  0.0646368641
## 167 -0.0419668471 0.02992783 -0.101435371  0.0159847384
## 169  0.0407369972 0.03064306 -0.020002510  0.1008080666
## 171  0.0294865574 0.03112332 -0.030878732  0.0908634502
## 174  0.0555496391 0.03064210 -0.005052429  0.1156709564
## 182 -0.0030501389 0.03016977 -0.061519511  0.0557097149
## 187 -0.0928485654 0.03045762 -0.153273455 -0.0331517988
## 189  0.0208720745 0.02987524 -0.036351492  0.0800451175
## 190 -0.0460836607 0.02938966 -0.103957647  0.0120001701
## 193  0.0133919907 0.02974683 -0.044206786  0.0712524179
## 194 -0.0505646082 0.02968959 -0.109177829  0.0079627649
## 201 -0.0032013624 0.03010277 -0.061884722  0.0553348505
## 204  0.0738597256 0.03037551  0.014342112  0.1337684944
## 205  0.0032839429 0.03023516 -0.056081998  0.0624517640
## 208  0.0091430117 0.02962450 -0.048072194  0.0663900436
## 209  0.0150688624 0.03021184 -0.044680640  0.0730996411
## 211 -0.0357768960 0.03030975 -0.095843576  0.0238881412
## 214  0.0640320375 0.02992741  0.006414144  0.1229996831
## 219  0.0290631966 0.03009980 -0.030381447  0.0878670253
## 222 -0.0417388150 0.03080808 -0.103718651  0.0175355597
## 223 -0.0130960797 0.03007637 -0.071932634  0.0467647865
## 229 -0.0343082284 0.02999811 -0.093465487  0.0247963398
## 
## , , time
## 
##          Estimate   Est.Error         Q2.5       Q97.5
## 6    9.377311e-05 0.004676583 -0.010000644 0.010511985
## 29  -1.153548e-03 0.004963017 -0.013411178 0.008519354
## 34  -1.769170e-03 0.005173704 -0.014863748 0.007000290
## 36   5.444860e-04 0.004930185 -0.009696871 0.012341529
## 37  -8.424642e-04 0.004503315 -0.012007411 0.007835417
## 48  -3.073597e-04 0.004606605 -0.011343886 0.009140841
## 53  -1.047232e-03 0.004746564 -0.013357798 0.007595611
## 54   8.281534e-04 0.004664752 -0.008335926 0.011899191
## 58   1.851231e-03 0.005351900 -0.006665139 0.015760457
## 61  -2.119557e-04 0.004748128 -0.010391836 0.010328625
## 66   2.408748e-03 0.004813433 -0.004707888 0.014762061
## 67  -7.129167e-05 0.004270370 -0.009639125 0.009164398
## 69  -6.616181e-04 0.004785759 -0.012237245 0.008832542
## 71  -8.483316e-04 0.005802250 -0.014860056 0.010729697
## 74   4.467841e-04 0.004076710 -0.008038499 0.009907422
## 75   1.499076e-03 0.005293163 -0.007795087 0.015061040
## 76   2.511775e-05 0.005297612 -0.011518695 0.011718856
## 78   1.516155e-03 0.004485118 -0.006214606 0.012724347
## 79   8.663710e-04 0.005119945 -0.009108883 0.013537077
## 80  -4.335669e-04 0.004631241 -0.010828324 0.009523502
## 81   1.524852e-03 0.004563816 -0.006378188 0.013139198
## 82   1.686775e-03 0.005177986 -0.007603765 0.014036800
## 85   3.116905e-04 0.005039373 -0.010172644 0.012030039
## 86   6.201037e-04 0.005319253 -0.010233882 0.013086501
## 87  -1.602485e-03 0.005943877 -0.016614821 0.007963270
## 89   7.371392e-04 0.005185634 -0.009147648 0.013554709
## 91  -2.273056e-04 0.004935378 -0.011371446 0.010294762
## 92  -2.439922e-04 0.004670379 -0.010996069 0.009727574
## 93  -1.191667e-03 0.005040713 -0.013999059 0.008052610
## 94  -3.979113e-04 0.005448832 -0.012843689 0.010918527
## 96   3.183740e-04 0.005044592 -0.010336261 0.011958721
## 97   5.840170e-04 0.004932806 -0.009552887 0.012090602
## 98  -1.975607e-03 0.005472160 -0.016705516 0.007014857
## 99   2.352301e-04 0.004668190 -0.009682632 0.010941811
## 101  2.337707e-04 0.004991775 -0.010732118 0.011399234
## 102 -2.578649e-04 0.004760780 -0.011059022 0.009649525
## 103  2.361689e-05 0.005168692 -0.011160132 0.011466215
## 104 -7.525999e-05 0.004933852 -0.010874215 0.011039687
## 105 -6.053050e-04 0.004862784 -0.011802988 0.009183947
## 106 -2.162985e-04 0.004779372 -0.010960333 0.010357368
## 110 -1.216037e-03 0.004766419 -0.013175701 0.007575598
## 112  1.253551e-03 0.005507079 -0.008708227 0.015095834
## 114  8.726187e-04 0.005030309 -0.008967576 0.013210855
## 115  1.925360e-03 0.005308664 -0.006553703 0.015722529
## 116  1.757060e-03 0.005400411 -0.006884217 0.016029328
## 120 -1.072852e-03 0.006381956 -0.016649540 0.010945900
## 122  5.866718e-04 0.004717024 -0.008927733 0.012009990
## 125  2.898243e-04 0.004859444 -0.009991086 0.011746358
## 127 -7.734943e-04 0.004941353 -0.012849690 0.008815386
## 129  3.855952e-04 0.005094742 -0.010225545 0.012360397
## 135  1.439907e-03 0.005623103 -0.008465729 0.015436224
## 136 -1.199183e-03 0.005592879 -0.015360705 0.009159188
## 137 -8.488514e-04 0.005630778 -0.014748775 0.009923352
## 140  2.664475e-04 0.005266877 -0.010927823 0.012370220
## 141  1.412885e-03 0.005197679 -0.007465533 0.014646471
## 142 -3.227368e-05 0.004679822 -0.010533274 0.010198988
## 143 -8.734615e-05 0.005460339 -0.012770923 0.011704893
## 144  9.515937e-04 0.005082078 -0.008422905 0.013664101
## 146  6.279387e-04 0.004788364 -0.008954343 0.012264544
## 149 -9.684403e-04 0.005366254 -0.014533235 0.008897237
## 150  3.985585e-04 0.004702308 -0.009589282 0.011243939
## 152 -7.981132e-04 0.005167430 -0.013386866 0.009480317
## 153 -1.405273e-03 0.004977952 -0.013943499 0.007293624
## 155  4.702192e-04 0.005062971 -0.009479955 0.012491756
## 156 -1.124572e-03 0.005104810 -0.014003466 0.008554146
## 159  4.646027e-04 0.005249506 -0.010019832 0.013161281
## 160  1.700891e-03 0.005274451 -0.006963717 0.015290038
## 162 -2.507540e-03 0.007472903 -0.021801979 0.009522140
## 163  3.912151e-05 0.004886594 -0.010677472 0.010996816
## 165 -9.996791e-04 0.004794309 -0.012608393 0.008105275
## 167 -8.019383e-04 0.004871943 -0.012674465 0.008800409
## 169 -2.327845e-04 0.004964060 -0.011514654 0.010133330
## 171 -7.341674e-04 0.004945396 -0.012835448 0.009195488
## 174 -9.917088e-04 0.005132908 -0.013398093 0.008567091
## 182  4.731708e-04 0.004838455 -0.009285501 0.011757877
## 187  3.419527e-05 0.005412784 -0.011611630 0.012073702
## 189  1.914802e-03 0.005504224 -0.006667377 0.016808134
## 190 -9.744613e-04 0.005134259 -0.013378599 0.008772970
## 193 -9.388601e-04 0.005111923 -0.013497614 0.008602034
## 194 -7.079629e-04 0.004997462 -0.012278605 0.009338836
## 201  5.980466e-04 0.004781526 -0.009082671 0.012213379
## 204 -1.542880e-03 0.005553189 -0.015998840 0.008017880
## 205 -6.900862e-04 0.004783444 -0.012124984 0.008693741
## 208  1.685347e-03 0.005206528 -0.006996321 0.014994337
## 209  1.262562e-03 0.005147761 -0.007733215 0.014342308
## 211 -1.271057e-03 0.005063699 -0.014048510 0.008036859
## 214 -3.304823e-04 0.005286124 -0.012535802 0.010633378
## 219  1.138312e-03 0.005182624 -0.008158486 0.014349770
## 222  1.371535e-03 0.005262340 -0.008148915 0.015150935
## 223 -2.336144e-03 0.005734800 -0.017828649 0.005915963
## 229 -1.844144e-03 0.005379918 -0.015962449 0.006948370
```

---
coef = fixef + raneff

```r
coef(mlm.4)
```

```
## $ID
## , , Intercept
## 
##       Estimate  Est.Error       Q2.5     Q97.5
## 6   0.20188402 0.03323076 0.13717333 0.2690105
## 29  0.15603191 0.03521972 0.08774925 0.2267315
## 34  0.14671945 0.03683005 0.07781887 0.2237222
## 36  0.17227515 0.03057602 0.11052335 0.2307020
## 37  0.20142185 0.03141744 0.14135965 0.2666698
## 48  0.22607882 0.03202312 0.16377111 0.2910473
## 53  0.20447421 0.03522589 0.13795098 0.2774407
## 54  0.24179525 0.03057940 0.18174555 0.3011711
## 58  0.22879747 0.02719820 0.17391009 0.2817091
## 61  0.12630926 0.03478343 0.05566292 0.1935265
## 66  0.23012884 0.02813154 0.17369906 0.2848994
## 67  0.20710265 0.02523635 0.15764740 0.2575477
## 69  0.20758361 0.02959041 0.14844377 0.2650707
## 71  0.09096862 0.02771993 0.03641130 0.1448934
## 74  0.20768740 0.02645122 0.15484393 0.2590562
## 75  0.25183411 0.03019587 0.19210772 0.3106481
## 76  0.12152656 0.02646260 0.06927489 0.1739201
## 78  0.23755295 0.02740914 0.18295842 0.2904702
## 79  0.14970955 0.03075918 0.08929074 0.2097061
## 80  0.16281025 0.03344223 0.09760776 0.2286258
## 81  0.23487279 0.02792578 0.17837904 0.2882850
## 82  0.30987805 0.02600603 0.25753412 0.3604075
## 85  0.16703562 0.02891595 0.11056527 0.2242362
## 86  0.13673035 0.02915207 0.07952230 0.1940986
## 87  0.13618365 0.02928275 0.07998624 0.1941604
## 89  0.14796584 0.02562888 0.09750570 0.1978650
## 91  0.17918371 0.02493079 0.13034255 0.2288642
## 92  0.20431859 0.02595562 0.15397024 0.2550830
## 93  0.24723248 0.02583910 0.19693718 0.2980091
## 94  0.13578087 0.02515823 0.08704243 0.1857850
## 96  0.22230716 0.02892620 0.16454255 0.2787601
## 97  0.24303191 0.02325636 0.19708702 0.2885783
## 98  0.15420749 0.02349899 0.10912331 0.2013007
## 99  0.18743144 0.02301073 0.14369171 0.2333966
## 101 0.23785355 0.02604562 0.18687327 0.2896512
## 102 0.19276921 0.02292310 0.14781130 0.2384735
## 103 0.16579381 0.02869389 0.10943213 0.2228564
## 104 0.18029952 0.02964297 0.12211568 0.2382874
## 105 0.17186534 0.02476637 0.12376956 0.2202090
## 106 0.20009014 0.02563773 0.15027133 0.2496048
## 110 0.19046619 0.02683940 0.13856460 0.2442159
## 112 0.11391265 0.02660398 0.06189243 0.1663276
## 114 0.14671047 0.02556429 0.09676805 0.1962601
## 115 0.15172080 0.02559289 0.10176650 0.2019613
## 116 0.15399137 0.02622182 0.10173984 0.2044029
## 120 0.31352776 0.03032740 0.25445966 0.3727998
## 122 0.17665984 0.02556034 0.12693992 0.2267448
## 125 0.19796007 0.02567301 0.14843410 0.2493948
## 127 0.22433260 0.02577243 0.17381118 0.2743994
## 129 0.14740517 0.02928264 0.09083250 0.2050302
## 135 0.27875859 0.02618155 0.22707609 0.3297192
## 136 0.27749077 0.02641164 0.22658211 0.3294014
## 137 0.26921995 0.02913224 0.21249344 0.3255794
## 140 0.14570383 0.02885911 0.08879713 0.2025160
## 141 0.19359720 0.02933425 0.13635469 0.2507349
## 142 0.18499731 0.02954388 0.12696391 0.2425400
## 143 0.28217299 0.02964185 0.22540630 0.3405650
## 144 0.18016346 0.02962237 0.12171900 0.2369020
## 146 0.20807988 0.02968790 0.14965889 0.2662954
## 149 0.22373530 0.03057349 0.16383599 0.2848620
## 150 0.20173130 0.02946883 0.14388659 0.2608469
## 152 0.21701815 0.02885096 0.15989235 0.2744973
## 153 0.16391134 0.02933929 0.10641411 0.2206718
## 155 0.15854669 0.03060683 0.09794287 0.2180123
## 156 0.15176788 0.02537125 0.10262448 0.2025534
## 159 0.14301276 0.02874507 0.08654755 0.1997256
## 160 0.18102152 0.03002860 0.12087126 0.2390872
## 162 0.36202739 0.02850237 0.30681011 0.4199658
## 163 0.15171172 0.02999481 0.09341425 0.2096490
## 165 0.20001941 0.02964909 0.14217052 0.2583536
## 167 0.15225964 0.02976887 0.09307730 0.2097754
## 169 0.23496349 0.03039040 0.17402040 0.2945245
## 171 0.22371305 0.03081910 0.16418559 0.2845551
## 174 0.24977613 0.03027891 0.18994388 0.3098302
## 182 0.19117635 0.03008083 0.13347125 0.2499220
## 187 0.10137792 0.03028420 0.04213389 0.1604828
## 189 0.21509856 0.02964895 0.15713648 0.2739412
## 190 0.14814283 0.02925301 0.09072097 0.2058448
## 193 0.20761848 0.02952696 0.14991945 0.2660538
## 194 0.14366188 0.02933366 0.08633740 0.2014048
## 201 0.19102513 0.02998416 0.13270538 0.2493391
## 204 0.26808621 0.03027467 0.20863534 0.3286696
## 205 0.19751043 0.03016489 0.13824983 0.2565522
## 208 0.20336950 0.02935436 0.14575838 0.2603627
## 209 0.20929535 0.02997696 0.14996537 0.2671011
## 211 0.15844959 0.03019515 0.09821945 0.2179569
## 214 0.25825853 0.02967261 0.20140968 0.3164906
## 219 0.22328968 0.02962725 0.16486152 0.2819754
## 222 0.15248767 0.03063407 0.09108214 0.2118115
## 223 0.18113041 0.02991882 0.12284861 0.2409011
## 229 0.15991826 0.02996095 0.10057667 0.2180268
## 
## , , time
## 
##          Estimate   Est.Error         Q2.5       Q97.5
## 6   -0.0027704939 0.005085273 -0.013589529 0.008026195
## 29  -0.0040178148 0.005495719 -0.017076099 0.006144566
## 34  -0.0046334372 0.005700413 -0.018593374 0.005062341
## 36  -0.0023197811 0.005325209 -0.013217156 0.009433029
## 37  -0.0037067312 0.004940008 -0.015202868 0.005390314
## 48  -0.0031716267 0.004984838 -0.014723661 0.006381549
## 53  -0.0039114994 0.005241969 -0.017135143 0.005292350
## 54  -0.0020361136 0.004998713 -0.011880585 0.009168044
## 58  -0.0010130359 0.005625993 -0.010404706 0.012836417
## 61  -0.0030762227 0.005194840 -0.014153769 0.007784317
## 66  -0.0004555190 0.005056079 -0.008717444 0.012012286
## 67  -0.0029355587 0.004668641 -0.013090465 0.006358018
## 69  -0.0035258852 0.005270288 -0.015802900 0.006150609
## 71  -0.0037125986 0.006290803 -0.018490044 0.008263203
## 74  -0.0024174830 0.004423487 -0.011687988 0.007023750
## 75  -0.0013651914 0.005560754 -0.011529966 0.011680463
## 76  -0.0028391493 0.005789838 -0.015501353 0.009417205
## 78  -0.0013481119 0.004762705 -0.009955014 0.009765143
## 79  -0.0019978960 0.005571217 -0.013133528 0.010721603
## 80  -0.0032978340 0.005108335 -0.014536146 0.006933302
## 81  -0.0013394152 0.004785227 -0.009927258 0.009799657
## 82  -0.0011774922 0.005367700 -0.010993302 0.011212139
## 85  -0.0025525766 0.005518500 -0.014260877 0.009448068
## 86  -0.0022441633 0.005732196 -0.013947787 0.010299523
## 87  -0.0044667522 0.006408206 -0.020601641 0.005800026
## 89  -0.0021271279 0.005623716 -0.013252015 0.010736414
## 91  -0.0030915726 0.005421456 -0.015028304 0.007773743
## 92  -0.0031082592 0.005133780 -0.014908908 0.007191925
## 93  -0.0040559341 0.005504287 -0.017814336 0.005706745
## 94  -0.0032621784 0.005939105 -0.016631071 0.008474930
## 96  -0.0025458930 0.005468062 -0.014080224 0.009297031
## 97  -0.0022802501 0.005282860 -0.013067081 0.009309298
## 98  -0.0048398745 0.006015270 -0.020634298 0.004714742
## 99  -0.0026290369 0.005097123 -0.013390890 0.008055860
## 101 -0.0026304963 0.005408116 -0.014460602 0.008903741
## 102 -0.0031221319 0.005232730 -0.015024326 0.007212572
## 103 -0.0028406501 0.005672272 -0.015082182 0.008867379
## 104 -0.0029395270 0.005410436 -0.014913597 0.008267825
## 105 -0.0034695720 0.005411321 -0.015765122 0.006882246
## 106 -0.0030805655 0.005198878 -0.014336285 0.007570628
## 110 -0.0040803041 0.005261721 -0.016876972 0.005087442
## 112 -0.0016107164 0.005886529 -0.012691352 0.012208739
## 114 -0.0019916484 0.005441680 -0.012923155 0.010445870
## 115 -0.0009389068 0.005628119 -0.010324536 0.012926614
## 116 -0.0011072070 0.005780080 -0.010958353 0.013347856
## 120 -0.0039371190 0.006730149 -0.020024831 0.008369847
## 122 -0.0022775953 0.005142406 -0.012829510 0.009230483
## 125 -0.0025744427 0.005249466 -0.013702233 0.008965531
## 127 -0.0036377613 0.005408725 -0.016736191 0.006179842
## 129 -0.0024786718 0.005562190 -0.014044609 0.009870700
## 135 -0.0014243600 0.005851357 -0.011986473 0.012264708
## 136 -0.0040634498 0.006009759 -0.019132042 0.006487644
## 137 -0.0037131184 0.006025519 -0.018030642 0.007375598
## 140 -0.0025978195 0.005733697 -0.014853310 0.009672324
## 141 -0.0014513824 0.005479800 -0.011354674 0.011598348
## 142 -0.0028965407 0.005205647 -0.014425749 0.007767373
## 143 -0.0029516132 0.005800231 -0.016202957 0.008932629
## 144 -0.0019126733 0.005448338 -0.012191968 0.010710987
## 146 -0.0022363284 0.005203815 -0.012696428 0.009470956
## 149 -0.0038327073 0.005818957 -0.018109401 0.006505454
## 150 -0.0024657085 0.005126019 -0.013379094 0.008537607
## 152 -0.0036623803 0.005628837 -0.017085494 0.006942508
## 153 -0.0042695400 0.005492235 -0.017850387 0.005084422
## 155 -0.0023940479 0.005524751 -0.013382520 0.009911847
## 156 -0.0039888390 0.005624767 -0.017560401 0.006489614
## 159 -0.0023996643 0.005680590 -0.014084374 0.010307430
## 160 -0.0011633757 0.005598347 -0.010705110 0.012756641
## 162 -0.0053718066 0.007782819 -0.025344417 0.007084947
## 163 -0.0028251455 0.005356471 -0.014443467 0.008470983
## 165 -0.0038639461 0.005315508 -0.016101415 0.005853019
## 167 -0.0036662053 0.005389758 -0.016334816 0.006560618
## 169 -0.0030970515 0.005374993 -0.015122082 0.007481683
## 171 -0.0035984345 0.005368475 -0.016395056 0.006684000
## 174 -0.0038559758 0.005545277 -0.016734513 0.006335463
## 182 -0.0023910962 0.005328898 -0.013167176 0.009322396
## 187 -0.0028300718 0.005888299 -0.015632948 0.009672398
## 189 -0.0009494652 0.005762006 -0.010411965 0.013719612
## 190 -0.0038387283 0.005659141 -0.017230159 0.006477029
## 193 -0.0038031271 0.005573264 -0.017021477 0.006453077
## 194 -0.0035722299 0.005520993 -0.016017326 0.007126293
## 201 -0.0022662205 0.005222673 -0.013024061 0.009383972
## 204 -0.0044071468 0.006013997 -0.019777407 0.005636236
## 205 -0.0035543533 0.005284598 -0.015658319 0.006350979
## 208 -0.0011789196 0.005512331 -0.010784346 0.012364678
## 209 -0.0016017049 0.005476442 -0.011578747 0.011450002
## 211 -0.0041353244 0.005584299 -0.018050899 0.005749083
## 214 -0.0031947493 0.005697382 -0.016169715 0.008009266
## 219 -0.0017259552 0.005510795 -0.011851639 0.011403540
## 222 -0.0014927316 0.005646395 -0.011858829 0.012294251
## 223 -0.0052004111 0.006194772 -0.021274901 0.003816297
## 229 -0.0047084111 0.005901407 -0.019813497 0.004925061
```

---

```r
mlm.5 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + time + (1 + time | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(gamma(3, .1), class = sd, coef = Intercept, group = ID), 
                prior(gamma(3, .1), class = sd, coef = time, group = ID), 
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      file = "mlm.5",
      data = mlm)
```

---
![](mlm-8_files/figure-html/unnamed-chunk-53-1.png)&lt;!-- --&gt;

---

```r
summary(mlm.5)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: CON ~ 1 + time + (1 + time | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)           0.06      0.01     0.05     0.08 1.00     3678     5793
## sd(time)                0.01      0.00     0.00     0.02 1.00     1504     3181
## cor(Intercept,time)    -0.25      0.31    -0.73     0.49 1.00     6780     5699
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.19      0.01     0.18     0.21 1.00     5215     7173
## time         -0.00      0.00    -0.01     0.00 1.00    10428     9687
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.05      0.00     0.04     0.05 1.00     2711     4691
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---


![](mlm-8_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
