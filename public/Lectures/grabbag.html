<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>grabbag</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="grabbag_files/header-attrs-2.5/header-attrs.js"></script>
    <link href="grabbag_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="grabbag_files/anchor-sections-1.0/anchor-sections.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">







&lt;style type="text/css"&gt;

.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}


.small { font-size: 80% }
.tiny { font-size: 55% }

&lt;/style&gt;


## This time

Hodgepodge:
1. Mediation &amp; multivariate/distribution models
2. Missing data
3. IRT


---
## Simple mediation

.pull-left[
`$$M  = i_M + a X + e_M$$`
`$$Y  = i_Y + c' X + b M + e_Y$$`

]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
]

---
## path analysis with lavaan



```r
library(lavaan)
```

```
## This is lavaan 0.6-7
```

```
## lavaan is BETA software! Please report any bugs.
```

```r
X &lt;- rnorm(100)
M &lt;- 0.5*X + rnorm(100)
Y &lt;- 0.7*M + rnorm(100)
Data &lt;- data.frame(X = X, Y = Y, M = M)
model &lt;- ' # direct effect
             Y ~ c*X
           # mediator
             M ~ a*X
             Y ~ b*M
           # indirect effect (a*b)
             ab := a*b
           # total effect
             total := c + (a*b)
         '
fit &lt;- sem(model, data = Data)
```


---

.tiny[

```r
summary(fit)
```

```
## lavaan 0.6-7 ended normally after 12 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          5
##                                                       
##   Number of observations                           100
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##   Y ~                                                 
##     X          (c)    0.168    0.118    1.428    0.153
##   M ~                                                 
##     X          (a)    0.445    0.106    4.197    0.000
##   Y ~                                                 
##     M          (b)    0.487    0.103    4.753    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##    .Y                 1.022    0.144    7.071    0.000
##    .M                 0.972    0.137    7.071    0.000
## 
## Defined Parameters:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##     ab                0.217    0.069    3.146    0.002
##     total             0.385    0.120    3.201    0.001
```

]

---


```r
library(blavaan)
  b.fit &lt;- bsem(model, data=Data, mcmcfile = "blavaan.1")
```

```
## 
## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.00041 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.1 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)
## Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)
## Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)
## Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)
## Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)
## Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)
## Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)
## Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)
## Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)
## Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)
## Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)
## Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.802134 seconds (Warm-up)
## Chain 1:                1.50085 seconds (Sampling)
## Chain 1:                2.30298 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000223 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.23 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)
## Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)
## Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)
## Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)
## Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)
## Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)
## Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)
## Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)
## Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)
## Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)
## Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)
## Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.814231 seconds (Warm-up)
## Chain 2:                1.53685 seconds (Sampling)
## Chain 2:                2.35108 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000211 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.11 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 1500 [  0%]  (Warmup)
## Chain 3: Iteration:  150 / 1500 [ 10%]  (Warmup)
## Chain 3: Iteration:  300 / 1500 [ 20%]  (Warmup)
## Chain 3: Iteration:  450 / 1500 [ 30%]  (Warmup)
## Chain 3: Iteration:  501 / 1500 [ 33%]  (Sampling)
## Chain 3: Iteration:  650 / 1500 [ 43%]  (Sampling)
## Chain 3: Iteration:  800 / 1500 [ 53%]  (Sampling)
## Chain 3: Iteration:  950 / 1500 [ 63%]  (Sampling)
## Chain 3: Iteration: 1100 / 1500 [ 73%]  (Sampling)
## Chain 3: Iteration: 1250 / 1500 [ 83%]  (Sampling)
## Chain 3: Iteration: 1400 / 1500 [ 93%]  (Sampling)
## Chain 3: Iteration: 1500 / 1500 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.764392 seconds (Warm-up)
## Chain 3:                1.53165 seconds (Sampling)
## Chain 3:                2.29604 seconds (Total)
## Chain 3: 
## Computing posterior predictives...
```

---
.tiny[

```r
summary(b.fit)
```

```
## blavaan (0.3-10) results of 1000 samples after 500 adapt/burnin iterations
## 
##   Number of observations                           100
## 
##   Number of missing patterns                         1
## 
##   Statistic                                 MargLogLik         PPP
##   Value                                       -310.715       0.529
## 
## Regressions:
##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
##   Y ~                                                                          
##     X          (c)    0.170    0.117   -0.058    0.406    1.000    normal(0,10)
##   M ~                                                                          
##     X          (a)    0.444    0.107    0.231    0.656    1.000    normal(0,10)
##   Y ~                                                                          
##     M          (b)    0.486    0.104    0.284    0.687    1.001    normal(0,10)
## 
## Intercepts:
##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
##    .Y                -0.186    0.103    -0.39    0.019    0.999    normal(0,10)
##    .M                -0.111    0.101   -0.304    0.079    1.000    normal(0,10)
## 
## Variances:
##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
##    .Y                 1.080    0.154     0.82    1.428    1.000 gamma(1,.5)[sd]
##    .M                 1.017    0.152    0.766    1.362    1.001 gamma(1,.5)[sd]
## 
## Defined Parameters:
##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
##     ab                0.216    0.071    0.077    0.355       NA                
##     total             0.386    0.122    0.146    0.625       NA
```

]

---
## brms

```r
# describe your equations
y_model &lt;- bf(Y ~ 1 + X + M)
m_model &lt;- bf(M ~ 1 + X)

# simultaneously estimate
med.1 &lt;-
  brm(family = gaussian,
      y_model + m_model + set_rescor(FALSE),
      data = Data,
      cores = 4,
      file = "med.1")
```


---
.small[

```r
summary(med.1)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: Y ~ 1 + X + M 
##          M ~ 1 + X 
##    Data: Data (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Y_Intercept    -0.17      0.09    -0.35     0.02 1.00     6324     2980
## M_Intercept     0.01      0.09    -0.16     0.18 1.00     5560     2698
## Y_X             0.01      0.12    -0.22     0.24 1.00     4839     2906
## Y_M             0.80      0.11     0.58     1.02 1.00     4620     3253
## M_X             0.51      0.10     0.32     0.71 1.00     5646     2485
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_Y     0.95      0.07     0.83     1.10 1.01     5487     2878
## sigma_M     0.90      0.06     0.79     1.04 1.00     6134     3151
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```
]


---
## Indirect effects


```r
library(tidybayes)
get_variables(med.1)
```

```
##  [1] "b_Y_Intercept" "b_M_Intercept" "b_Y_X"         "b_Y_M"        
##  [5] "b_M_X"         "sigma_Y"       "sigma_M"       "lp__"         
##  [9] "accept_stat__" "stepsize__"    "treedepth__"   "n_leapfrog__" 
## [13] "divergent__"   "energy__"
```


```r
med.1 %&gt;% 
  spread_draws(b_Y_M, b_M_X,b_Y_X) 
```

```
## # A tibble: 4,000 x 6
##    .chain .iteration .draw b_Y_M b_M_X    b_Y_X
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1      1          1     1 0.748 0.552  0.00151
##  2      1          2     2 0.679 0.388 -0.0409 
##  3      1          3     3 0.985 0.637 -0.0148 
##  4      1          4     4 0.702 0.419  0.113  
##  5      1          5     5 0.530 0.535  0.182  
##  6      1          6     6 0.487 0.588  0.201  
##  7      1          7     7 0.569 0.653  0.0600 
##  8      1          8     8 1.02  0.358 -0.0793 
##  9      1          9     9 0.767 0.697 -0.0121 
## 10      1         10    10 0.886 0.407 -0.0412 
## # … with 3,990 more rows
```


---
## calculate indirect effects

```r
med.1 %&gt;% 
  spread_draws(b_Y_M, b_M_X, b_Y_X) %&gt;% 
  mutate(indirect = b_Y_M * b_M_X) %&gt;% 
  mutate(direct = b_Y_X) %&gt;% 
  mutate(total = indirect + direct ) %&gt;% 
  median_qi(indirect, direct,total)
```

```
## # A tibble: 1 x 12
##   indirect indirect.lower indirect.upper  direct direct.lower direct.upper total
##      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1    0.407          0.234          0.616 0.00827       -0.221        0.241 0.423
## # … with 5 more variables: total.lower &lt;dbl&gt;, total.upper &lt;dbl&gt;, .width &lt;dbl&gt;,
## #   .point &lt;chr&gt;, .interval &lt;chr&gt;
```


---
.pull-left[

```r
med.1 %&gt;% 
  spread_draws(b_Y_M, b_M_X, b_Y_X) %&gt;% 
  mutate(indirect = b_Y_M * b_M_X) %&gt;% 
  mutate(direct = b_Y_X) %&gt;% 
  mutate(total = indirect + direct ) %&gt;% 
  select(indirect, direct, total) %&gt;% 
  gather() %&gt;% 
  ggplot(aes(y = key, x = value)) +
  stat_dotsinterval()
```
]
.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]

---
## priors for mediation
7 parameters to estimate


```r
get_prior(y_model + m_model + set_rescor(FALSE),
          family = gaussian,
          data = Data)
```

```
##                      prior     class coef group resp dpar nlpar bound
## 1                                  b                                 
## 2                          Intercept                                 
## 3                                  b               M                 
## 4                                  b    X          M                 
## 5  student_t(3, -0.1, 2.5) Intercept               M                 
## 6     student_t(3, 0, 2.5)     sigma               M                 
## 7                                  b               Y                 
## 8                                  b    M          Y                 
## 9                                  b    X          Y                 
## 10 student_t(3, -0.2, 2.5) Intercept               Y                 
## 11    student_t(3, 0, 2.5)     sigma               Y
```


---

```r
med.2 &lt;-
  brm(family = gaussian,
      y_model + m_model + set_rescor(FALSE),
       prior = c(prior(normal(0, 1), class = Intercept, resp = M),
                 prior(normal(0, 1), class = Intercept, resp = Y),
                prior(normal(0, 2), class = b, coef = X, resp = M),
                prior(normal(0, 2), class = b, coef = M, resp = Y),
                prior(normal(0, 2), class = b, coef = X, resp = Y),
                prior(exponential(1), class = sigma, resp = M),
                prior(exponential(1), class = sigma, resp = Y)),
      data = Data,
      cores = 4,
      file = "med.2")
```

---

```r
summary(med.2)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: Y ~ 1 + X + M 
##          M ~ 1 + X 
##    Data: Data (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Y_Intercept    -0.08      0.11    -0.29     0.13 1.00     5503     3182
## M_Intercept    -0.24      0.10    -0.43    -0.04 1.00     6293     2706
## Y_X            -0.19      0.12    -0.43     0.04 1.00     4824     3017
## Y_M             0.82      0.11     0.60     1.03 1.00     4907     3291
## M_X             0.47      0.10     0.27     0.66 1.00     5753     2991
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_Y     1.06      0.08     0.92     1.22 1.00     5604     3321
## sigma_M     0.97      0.07     0.84     1.12 1.00     6127     2984
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---


```r
med.2 %&gt;% 
  spread_draws(b_Y_M, b_M_X) %&gt;% 
  mutate(indirect = b_Y_M * b_M_X) %&gt;% 
  median_qi(indirect)
```

```
## # A tibble: 1 x 6
##   indirect .lower .upper .width .point .interval
##      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1    0.376  0.207  0.581   0.95 median qi
```



```r
med.1 %&gt;% 
  spread_draws(b_Y_M, b_M_X) %&gt;% 
  mutate(indirect = b_Y_M * b_M_X) %&gt;% 
  median_qi(indirect)
```

```
## # A tibble: 1 x 6
##   indirect .lower .upper .width .point .interval
##      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1    0.407  0.234  0.616   0.95 median qi
```

---
## Multiple Predictors, mediators and outcomes
.pull-left[

```r
n &lt;- 1e3
set.seed(4.5)
mult.X &lt;-
  tibble(X1 = rnorm(n, mean = 0, sd = 1),
         X2 = rnorm(n, mean = 0, sd = 1),
         X3 = rnorm(n, mean = 0, sd = 1)) %&gt;% 
  mutate(med = rnorm(n, mean = 0 + X1 * -1 + X2 * 0 + X3 * 1, sd = 1),
         dv  = rnorm(n, mean = 0 + X1 * 0 + X2 * .5 + X3 * 1 + M * .5, sd = 1))
```
]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]

---


```r
med.3 &lt;-
  brm(family = gaussian,
      bf(dv ~ 1 + X1 + X2 + X3 + med) + 
        bf(med ~ 1 + X1 + X2 + X3) + 
        set_rescor(FALSE),
      data = mult.X, 
      file = "med.3", 
      cores = 4)
```

---
.small[

```r
summary(med.3)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: dv ~ 1 + X1 + X2 + X3 + med 
##          med ~ 1 + X1 + X2 + X3 
##    Data: mult.X (Number of observations: 1000) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## dv_Intercept     -0.06      0.04    -0.13     0.01 1.00     7463     3520
## med_Intercept     0.00      0.03    -0.06     0.06 1.00     6236     3422
## dv_X1             0.01      0.05    -0.08     0.11 1.00     2985     3070
## dv_X2             0.56      0.04     0.49     0.63 1.00     6708     3193
## dv_X3             1.01      0.05     0.91     1.11 1.00     3010     2789
## dv_med           -0.04      0.04    -0.11     0.03 1.00     2529     2810
## med_X1           -0.93      0.03    -0.99    -0.87 1.00     6961     3375
## med_X2            0.03      0.03    -0.03     0.09 1.00     7103     3271
## med_X3            0.98      0.03     0.92     1.04 1.00     5887     3144
## 
## Family Specific Parameters: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_dv      1.13      0.03     1.08     1.18 1.00     6880     3095
## sigma_med     0.97      0.02     0.93     1.02 1.00     6630     2882
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```
]

---
## Multiple Outcomes
.pull-left[

```r
n &lt;- 1e3

set.seed(4.5)
Ys &lt;-
  tibble(X  = rnorm(n, mean = 0, sd = 1)) %&gt;% 
  mutate(M = rnorm(n, mean = 0 + X * .5, sd = 1)) %&gt;% 
  mutate(Y1 = rnorm(n, mean = 0 + X * -1 + M * 0,  sd = 1),
         Y2 = rnorm(n, mean = 0 + X * 0  + M * .5, sd = 1),
         Y3 = rnorm(n, mean = 0 + X * 1  + M * 1,  sd = 1))
```
]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]

---

```r
mult.Ys &lt;-
  brm(family = gaussian,
      bf(Y1 ~ 1 + X + M) + 
        bf(Y2 ~ 1 + X + M) + 
        bf(Y3 ~ 1 + X + M) + 
        bf(M ~ 1 + X) + 
        set_rescor(FALSE),
      data = Ys, 
      cores = 4,
      fit = "med.4")
```

```
## Compiling Stan program...
```

```
## Trying to compile a simple C file
```

```
## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include" -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name 'namespace'
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected ';' after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
```

```
## Start sampling
```

---

.tiny[

```r
summary(mult.Ys)
```

```
##  Family: MV(gaussian, gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: Y1 ~ 1 + X + M 
##          Y2 ~ 1 + X + M 
##          Y3 ~ 1 + X + M 
##          M ~ 1 + X 
##    Data: Ys (Number of observations: 1000) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Y1_Intercept     0.01      0.03    -0.05     0.07 1.00    10293     2935
## Y2_Intercept     0.00      0.03    -0.06     0.06 1.00     8168     3069
## Y3_Intercept    -0.01      0.03    -0.07     0.05 1.00     9527     2920
## M_Intercept      0.03      0.03    -0.03     0.09 1.00    10131     3256
## Y1_X            -1.05      0.04    -1.12    -0.98 1.00     6745     3227
## Y1_M             0.05      0.03    -0.01     0.11 1.00     6361     3356
## Y2_X             0.05      0.04    -0.01     0.12 1.00     6099     3330
## Y2_M             0.53      0.03     0.47     0.59 1.00     7032     3157
## Y3_X             1.03      0.04     0.96     1.10 1.00     7668     3123
## Y3_M             1.06      0.03     1.00     1.12 1.00     7737     3702
## M_X              0.53      0.03     0.47     0.59 1.00     8934     3095
## 
## Family Specific Parameters: 
##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_Y1     0.98      0.02     0.93     1.02 1.00    10207     2772
## sigma_Y2     0.97      0.02     0.93     1.02 1.00     9525     2580
## sigma_Y3     1.00      0.02     0.96     1.05 1.01    11919     2840
## sigma_M      1.00      0.02     0.96     1.04 1.00     9375     2575
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```
]

---
## Multiple Mediators
Note we can compute individual and total indirect effects

![](grabbag_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;

---
## parallel

```r
m1_model &lt;- bf(M1 ~ 1 + X)
m2_model &lt;- bf(M2  ~ 1 + X)
y_model  &lt;- bf(Y ~ 1 + X + M1 + M2)

par &lt;-
  brm(family = gaussian,
      y_model + m1_model + m2_model + set_rescor(FALSE),
      data = d)
```
3 indirect effects can be calculated
---
## serial

```r
ser &lt;-
  brm(family = gaussian,
        bf(M1 ~ 1 + X) + 
        bf(M2 ~ 1 + X + M1) + 
        bf(Y ~ 1 + X + M1 + M2) + 
        set_rescor(FALSE),
        data=d)
```

4 indirect effects can be calculated

---
## moderated mediation? 

```r
y_model &lt;- bf(Y ~ 1 + X + M)
m_model &lt;- bf(M ~ 1 + X*moderator)

med.5 &lt;-
  brm(family = gaussian,
      y_model + m_model + set_rescor(FALSE),
      data = Data,
      cores = 4,
      file = "med.5")
```

---
## Multivariate models
The mediation models presented above are examples of multivariate models. What are multivariate models? Any model that has more than 1 DV. While common within SEM frameworks, multivariate models are not often used within standard linear modeling, mostly because of computational difficulties. 

When do you want to use multivariate models? All the time! Mediation, path models, distributional models, IRT models, parallel process MLMs, etc etc. 

What are advantages? Fewer models than doing separate, additional parameters, novel Qs. 


---
## multivariate MLMs


.pull-left[

```r
mlm.4 &lt;- 
  brm(family = gaussian,
      CON ~ 1 + time + (1 + time | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(normal(0, 1.5), class = sd, coef = Intercept, group = ID), 
                prior(normal(0, 1.5), class = sd, coef = time, group = ID), 
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      file = "mlm.4",
      data = mlm)
```
]

.pull-right[

```r
mv.1 &lt;- 
  brm(family = gaussian,
      mvbind(CON, DAN) ~ 1 + time + (1 + time | ID),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1.5), class = b),
                prior(lkj(2), class = cor),
                prior(lkj(2), class = rescor)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      file = "mv.1",
      data = mlm)
```
]

---
.small[

```r
summary(mv.1)
```

```
## Warning: There were 103 divergent transitions after warmup. Increasing
## adapt_delta above 0.8 may help. See http://mc-stan.org/misc/
## warnings.html#divergent-transitions-after-warmup
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: CON ~ 1 + time + (1 + time | ID) 
##          DAN ~ 1 + time + (1 + time | ID) 
##    Data: mlm (Number of observations: 225) 
## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;
##          total post-warmup samples = 12000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 91) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(CON_Intercept)               0.06      0.01     0.05     0.07 1.00     2999
## sd(CON_time)                    0.00      0.00     0.00     0.01 1.00     1145
## sd(DAN_Intercept)               0.04      0.01     0.03     0.05 1.00     2098
## sd(DAN_time)                    0.00      0.00     0.00     0.01 1.01     1001
## cor(CON_Intercept,CON_time)    -0.12      0.39    -0.79     0.70 1.00     4537
## cor(DAN_Intercept,DAN_time)    -0.04      0.41    -0.75     0.77 1.00     5895
##                             Tail_ESS
## sd(CON_Intercept)               4362
## sd(CON_time)                    1297
## sd(DAN_Intercept)               1609
## sd(DAN_time)                     781
## cor(CON_Intercept,CON_time)     2874
## cor(DAN_Intercept,DAN_time)     5598
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## CON_Intercept     0.19      0.01     0.18     0.21 1.00     4259     6190
## DAN_Intercept     0.20      0.01     0.19     0.22 1.00     4537     4876
## CON_time         -0.00      0.00    -0.01     0.00 1.00    10046     9297
## DAN_time         -0.01      0.00    -0.01    -0.00 1.00     9678     8426
## 
## Family Specific Parameters: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_CON     0.05      0.00     0.04     0.05 1.00     2714     2520
## sigma_DAN     0.05      0.00     0.04     0.06 1.00     1939      985
## 
## Residual Correlations: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(CON,DAN)     0.12      0.09    -0.06     0.29 1.00     5116     6552
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```
]

---

```r
fixef(mv.1)
```

```
##                   Estimate   Est.Error         Q2.5         Q97.5
## CON_Intercept  0.194580508 0.007714619  0.179321416  0.2097078801
## DAN_Intercept  0.203559478 0.006649685  0.190714378  0.2166930010
## CON_time      -0.002892927 0.002258469 -0.007330227  0.0015582881
## DAN_time      -0.005195106 0.002205743 -0.009563482 -0.0009330466
```


```r
mv.1 %&gt;% 
  spread_draws(rescor__CON__DAN) %&gt;% 
   median_qi()
```

```
## # A tibble: 1 x 6
##   rescor__CON__DAN  .lower .upper .width .point .interval
##              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1            0.119 -0.0555  0.292   0.95 median qi
```

---
## Distributional Models

In basic regression with a Gaussian DV, we predict the mean, `\(\mu\)` through some linear model. The second parameter of the normal distribution – the residual standard deviation `\(\sigma\)` – is assumed to be constant across observations. We estimate it but do not try to predict it.

This extends beyond Gaussian DVs, as most response distributions have a "location" parameter and one or more "scale" or "shape" parameters. Instead of only predicting the location parameters, we can also predict the scale parameters

When to use? Well, you've seen this with Welch's t-test, and if you've ever done SEM you can model variance differences with group models all the time. 

---

`$$y_{ik} \sim t(\mu_{ik}, \sigma_{ik})$$`
`$$\mu_{ik} = \beta_0 + \beta_1 Group_{ik}$$`
`$$\sigma_{ik} = \gamma_0 + \gamma_1 Group_{ik}$$`

---

```r
week3 &lt;- "https://raw.githubusercontent.com/josh-jackson/bayes/master/week3.csv"
welch &lt;- read.csv(week3)
welch &lt;- welch %&gt;% 
mutate(mood.group.d = recode(mood.group, 
                             '0' = "control", 
                             '1' = "tx", 
                             '2' = "tx")) 

d.1 &lt;- 
  brm(family = student,
     bf( health ~ 0 + mood.group.d,
         sigma ~ 0 + mood.group.d),
                file = "d.1",
                data = welch)
```

---

```r
summary(d.1)
```

```
##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: health ~ 0 + mood.group.d 
##          sigma ~ 0 + mood.group.d
##    Data: welch (Number of observations: 200) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## mood.group.dcontrol           7.99      0.34     7.33     8.68 1.00     4361
## mood.group.dtx                8.77      0.24     8.31     9.25 1.00     4683
## sigma_mood.group.dcontrol     1.01      0.09     0.84     1.20 1.00     4590
## sigma_mood.group.dtx          1.00      0.07     0.87     1.14 1.00     4411
##                           Tail_ESS
## mood.group.dcontrol           2697
## mood.group.dtx                2933
## sigma_mood.group.dcontrol     3175
## sigma_mood.group.dtx          3194
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## nu    29.30     14.84     9.82    66.05 1.00     4313     3116
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---


```r
d.2 &lt;- 
  brm(family = student,
     bf( health ~ 0 + mood.group.d,
         sigma ~ 0 + mood.group.d*SES),
                file = "d.2",
                data = welch)
```

---


```r
summary(d.2)
```

```
##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: health ~ 0 + mood.group.d 
##          sigma ~ 0 + mood.group.d * SES
##    Data: welch (Number of observations: 200) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## mood.group.dcontrol           7.97      0.37     7.25     8.70 1.00     4091
## mood.group.dtx                8.73      0.25     8.25     9.23 1.00     4273
## sigma_mood.group.dcontrol     1.03      0.10     0.84     1.22 1.00     4708
## sigma_mood.group.dtx          0.99      0.07     0.85     1.12 1.00     4232
## sigma_SES                     0.02      0.10    -0.18     0.22 1.00     2080
## sigma_mood.group.dtx:SES      0.07      0.12    -0.17     0.32 1.00     2210
##                           Tail_ESS
## mood.group.dcontrol           2934
## mood.group.dtx                2546
## sigma_mood.group.dcontrol     3012
## sigma_mood.group.dtx          3201
## sigma_SES                     2815
## sigma_mood.group.dtx:SES      2735
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## nu    29.10     14.88     9.61    65.50 1.00     3637     3283
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
## MELSM
.pull-left[
Mixed effects location scale model

```r
library(readr)
melsm &lt;- read_csv("melsm.csv") %&gt;% 
  mutate(day01 = (day - 2) / max((day - 2)))
```

```
## Parsed with column specification:
## cols(
##   X1 = col_double(),
##   P_A.std = col_double(),
##   day = col_double(),
##   P_A.lag = col_double(),
##   N_A.lag = col_double(),
##   steps.pm = col_double(),
##   steps.pmd = col_double(),
##   record_id = col_double(),
##   N_A.std = col_double()
## )
```
]

.pull-right[

```r
head(melsm)
```

```
## # A tibble: 6 x 10
##      X1 P_A.std   day P_A.lag N_A.lag steps.pm steps.pmd record_id N_A.std
##   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1     2   1.75      2   0.748   0.254    0.955     0.600         1 -0.734 
## 2     3  -0.231     3   1.47   -0.854    0.955    -0.395         1  0.539 
## 3     4   0.342     4  -0.377   0.961    0.955    -1.52          1  0.602 
## 4     5   0.457     5   0.129  -0.196    0.955    -1.34          1  0.278 
## 5     6  -0.235     6   0.329  -0.160    0.955     0.418         1  0.547 
## 6     7   1.13      7   1.41   -0.904    0.955    -0.323         1  0.0566
## # … with 1 more variable: day01 &lt;dbl&gt;
```


```r
melsm %&gt;% 
distinct(record_id) %&gt;% 
  count()
```

```
## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1   193
```
]

---
.pull-left[
Participants filled out daily affective measures and physical activity


```r
melsm %&gt;% 
    count(record_id) %&gt;% 
  ggplot(aes(x = n)) +
  geom_bar() +
  scale_x_continuous("number of days", limits = c(0, NA))
```
]

.pull-right[

![](grabbag_files/figure-html/unnamed-chunk-44-1.png)&lt;!-- --&gt;


]

---
Participant level

.pull-left[

```r
melsm %&gt;% 
  nest(data = c(X1, P_A.std, day, P_A.lag, N_A.lag, steps.pm, steps.pmd, N_A.std, day01)) %&gt;% 
  slice_sample(n = 16) %&gt;% 
  unnest(data) %&gt;% 
  ggplot(aes(x = day, y = N_A.lag)) +
  geom_line(color = "black") +
  geom_point(color = "black", size = 1/2) +
  ylab("negative affect (standardized)") +
  facet_wrap(~record_id)
```
]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;
]


---
## Standard MLM treatment


```r
melsm.1 &lt;-
  brm(family = gaussian,
      N_A.std ~ 1 + day01 + (1 + day01 | record_id),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.1")
```


---


```r
summary(melsm.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: N_A.std ~ 1 + day01 + (1 + day01 | record_id) 
##    Data: melsm (Number of observations: 13033) 
## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~record_id (Number of levels: 193) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)            0.77      0.04     0.70     0.86 1.00     1447
## sd(day01)                0.65      0.05     0.57     0.75 1.00     2782
## cor(Intercept,day01)    -0.34      0.08    -0.49    -0.18 1.00     2855
##                      Tail_ESS
## sd(Intercept)            2864
## sd(day01)                4965
## cor(Intercept,day01)     4221
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.03      0.06    -0.08     0.15 1.00      707     1450
## day01        -0.16      0.06    -0.27    -0.05 1.00     2426     4367
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.61      0.00     0.60     0.62 1.00    14365     4831
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
## MLM assumptions

.pull-left[
Sigma, which captures the variation in NA not accounted for by the intercepts, time predictors, and the correlation. An assumption is that sigma does NOT vary across persons, occasions, or other variables. 

Posterior predictive interval is the same (and fitted) even though it seem inappropriate from person to person
]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;
]



---

.tiny[

`$$NA_{ij} \sim \operatorname{Normal}(\mu_{ij}, \sigma_{i})$$`

`$$\mu_{ij}  = \beta_0 + \beta_1 time_{ij} + u_{0i} + u_{1i} time_{ij}$$`
`$$\log(\sigma_i )  = \eta_0 + u_{2i}$$`

`$$\begin{bmatrix} u_{0i} \\ u_{1i} \\ {u_{2i}} \end{bmatrix}  \sim \operatorname{MVNormal}\begin{pmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf S \mathbf R \mathbf S \end{pmatrix}$$`
`$$\mathbf S  = \begin{bmatrix} \sigma_0 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_1 &amp; 0 \\ 0 &amp; 0 &amp; \sigma_2 \end{bmatrix}$$`
`$$\mathbf R = \begin{bmatrix} 1 &amp; \rho_{12} &amp; \rho_{13} \\ \rho_{21} &amp; 1 &amp; \rho_{23} \\ \rho_{31} &amp; \rho_{32} &amp; 1 \end{bmatrix}$$`
`$$\beta_0  \sim \operatorname{Normal}(0, 0.2)$$`
$$\beta_1 \text{and } \eta_0  \sim \operatorname{Normal}(0, 1) $$
$$ \sigma_0,\dots, \sigma_2 \sim \operatorname{Exponential}(1) $$
`$$\mathbf R  \sim \operatorname{LKJ}(2)$$`
]

---

note: 1) brms default is to use log-link when modeling sigma
2) |i| syntax within the parentheses allow for correlated random effects. Without this, the random intercept and slope would not correlated with the random sigma term, effectively setting the correlation equal to zero 

```r
melsm.2 &lt;-
  brm(family = gaussian,
      bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),
         sigma ~ 1 + (1 |i| record_id)),
                prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(normal(0, 1), class = Intercept, dpar = sigma),
                prior(exponential(1), class = sd, dpar = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.2")
```

---

.small[


```r
summary(melsm.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) 
##          sigma ~ 1 + (1 | i | record_id)
##    Data: melsm (Number of observations: 13033) 
## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~record_id (Number of levels: 193) 
##                                Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                      0.76      0.04     0.69     0.84 1.00
## sd(day01)                          0.60      0.04     0.52     0.69 1.00
## sd(sigma_Intercept)                0.69      0.04     0.63     0.77 1.00
## cor(Intercept,day01)              -0.33      0.08    -0.49    -0.17 1.00
## cor(Intercept,sigma_Intercept)     0.61      0.05     0.51     0.70 1.01
## cor(day01,sigma_Intercept)        -0.10      0.08    -0.26     0.06 1.01
##                                Bulk_ESS Tail_ESS
## sd(Intercept)                       708     1265
## sd(day01)                          1644     3365
## sd(sigma_Intercept)                 694     1520
## cor(Intercept,day01)                735     1374
## cor(Intercept,sigma_Intercept)      800     1466
## cor(day01,sigma_Intercept)          542     1343
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept           0.04      0.05    -0.07     0.14 1.01      283      574
## sigma_Intercept    -0.78      0.05    -0.88    -0.68 1.00      339      713
## day01              -0.16      0.05    -0.26    -0.05 1.00      477     1023
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```
]

---


```r
melsm.2 %&gt;% 
  spread_draws(b_sigma_Intercept) %&gt;% 
  exp() %&gt;% 
  median_qi()
```

```
## # A tibble: 1 x 6
##   b_sigma_Intercept .lower .upper .width .point .interval
##               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1             0.459  0.413  0.507   0.95 median qi
```

---


```r
melsm.2 %&gt;% 
spread_draws(b_sigma_Intercept,r_record_id__sigma[ID, term]) 
```

```
## # A tibble: 1,544,000 x 7
## # Groups:   ID, term [193]
##    .chain .iteration .draw b_sigma_Intercept    ID term      r_record_id__sigma
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;             &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;
##  1      1          1     1            -0.773     1 Intercept              0.400
##  2      1          1     1            -0.773     2 Intercept             -1.23 
##  3      1          1     1            -0.773     3 Intercept              0.246
##  4      1          1     1            -0.773     4 Intercept              0.659
##  5      1          1     1            -0.773     5 Intercept             -0.963
##  6      1          1     1            -0.773     6 Intercept              0.306
##  7      1          1     1            -0.773     7 Intercept              0.103
##  8      1          1     1            -0.773     8 Intercept             -0.973
##  9      1          1     1            -0.773     9 Intercept             -0.549
## 10      1          1     1            -0.773    10 Intercept             -0.322
## # … with 1,543,990 more rows
```

8000 samples * 193 participants = 1544000

---

.pull-left[

```r
melsm.2 %&gt;% 
spread_draws(b_sigma_Intercept,r_record_id__sigma[ID, term]) %&gt;% 
  mutate(b_sigma_Intercept = exp(b_sigma_Intercept)) %&gt;% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) %&gt;% 
   median_qi(estimate = b_sigma_Intercept + r_record_id__sigma) %&gt;% 
  ggplot(aes(x = reorder(ID, estimate), y = estimate, ymin = .lower, ymax = .upper)) +
   geom_pointinterval(point_colour = "black", interval_color = "grey", point_alpha = .25) + scale_x_discrete("Participants ranked by posterior SD", breaks = NULL) + ylab("sigma estimate") + theme_light()
```
]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;
]

---
.pull-left[

```r
fits2 &lt;- newd %&gt;%
  add_fitted_draws(melsm.2)

preds2 &lt;- newd %&gt;%
  add_predicted_draws(melsm.2)

fits2 %&gt;% 
ggplot(aes(x = day01, y = N_A.std)) +
  stat_lineribbon(aes(y = .value),.width = c(.95), alpha = 1/4, color ="grey") +
  stat_lineribbon(data = preds2, aes(y = .prediction),.width = c(.90), alpha = 1/4, color ="blue") +
  geom_point(data = newd) +
  facet_wrap(~record_id)
```

]

![](grabbag_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;




---
### time as a predictor of sigma


```r
melsm.3 &lt;-
  brm(family = gaussian,
      bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),
         sigma ~ 1 + day01 + (1 + day01 |i| record_id)),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(normal(0, 1), class = Intercept, dpar = sigma),
                prior(normal(0, 1), class = b, dpar = sigma),
                prior(exponential(1), class = sd, dpar = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.3")
```


---

.small[



```r
summary(melsm.3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) 
##          sigma ~ 1 + day01 + (1 + day01 | i | record_id)
##    Data: melsm (Number of observations: 13033) 
## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~record_id (Number of levels: 193) 
##                                  Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                        0.76      0.04     0.68     0.84 1.01
## sd(day01)                            0.60      0.04     0.52     0.69 1.00
## sd(sigma_Intercept)                  0.70      0.04     0.63     0.78 1.00
## sd(sigma_day01)                      0.36      0.04     0.29     0.44 1.00
## cor(Intercept,day01)                -0.31      0.08    -0.47    -0.14 1.00
## cor(Intercept,sigma_Intercept)       0.64      0.05     0.55     0.72 1.00
## cor(day01,sigma_Intercept)          -0.20      0.08    -0.36    -0.04 1.00
## cor(Intercept,sigma_day01)          -0.16      0.11    -0.36     0.05 1.00
## cor(day01,sigma_day01)               0.61      0.09     0.42     0.76 1.00
## cor(sigma_Intercept,sigma_day01)    -0.15      0.10    -0.33     0.04 1.00
##                                  Bulk_ESS Tail_ESS
## sd(Intercept)                         539     1690
## sd(day01)                            2261     3983
## sd(sigma_Intercept)                  1633     3157
## sd(sigma_day01)                      4420     6391
## cor(Intercept,day01)                 1330     1961
## cor(Intercept,sigma_Intercept)       1470     3460
## cor(day01,sigma_Intercept)           1262     1962
## cor(Intercept,sigma_day01)           4779     6053
## cor(day01,sigma_day01)               3928     4856
## cor(sigma_Intercept,sigma_day01)     3950     5666
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept           0.03      0.05    -0.08     0.13 1.03      382      692
## sigma_Intercept    -0.75      0.05    -0.86    -0.65 1.01      716     1626
## day01              -0.15      0.05    -0.26    -0.05 1.01     1224     2634
## sigma_day01        -0.11      0.04    -0.18    -0.03 1.00     3306     5612
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

]

---

.pull-left[

```r
fits3 &lt;- newd %&gt;%
  add_fitted_draws(melsm.3)

preds3 &lt;- newd %&gt;%
  add_predicted_draws(melsm.3)

fits3 %&gt;% 
ggplot(aes(x = day01, y = N_A.std)) +
  stat_lineribbon(aes(y = .value),.width = c(.95), alpha = 1/4, color ="grey") +
  stat_lineribbon(data = preds3, aes(y = .prediction),.width = c(.90), alpha = 1/4, color ="blue") +
  geom_point(data = newd) +
  facet_wrap(~record_id)
```

]

.pull-right[
![](grabbag_files/figure-html/unnamed-chunk-61-1.png)&lt;!-- --&gt;

]



---
### Multivariate MELSM


```r
melsm.4 &lt;-
  brm(family = gaussian,
      bf(mvbind(N_A.std, P_A.std) ~ 1 + day01 + (1 + day01 |i| record_id),
         sigma ~ 1 + day01 + (1 + day01 |i| record_id)) + set_rescor(rescor = FALSE),
      prior = c(prior(normal(0, 0.2), class = Intercept, resp = NAstd),
                prior(normal(0, 1), class = b, resp = NAstd),
                prior(exponential(1), class = sd, resp = NAstd),
                prior(normal(0, 1), class = Intercept, dpar = sigma, resp = NAstd),
                prior(normal(0, 1), class = b, dpar = sigma, resp = NAstd),
                prior(exponential(1), class = sd, dpar = sigma, resp = NAstd),
                prior(normal(0, 0.2), class = Intercept, resp = PAstd),
                prior(normal(0, 1), class = b, resp = PAstd),
                prior(exponential(1), class = sd, resp = PAstd),
                prior(normal(0, 1), class = Intercept, dpar = sigma, resp = PAstd),
                prior(normal(0, 1), class = b, dpar = sigma, resp = PAstd),
                prior(exponential(1), class = sd, dpar = sigma, resp = PAstd),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.4")
```

---

.small[


```r
summary(melsm.4)
```

]



---


```r
levels &lt;- c("beta[0]^'NA'", "beta[1]^'NA'", "eta[0]^'NA'", "eta[1]^'NA'",
            "beta[0]^'PA'", "beta[1]^'PA'", "eta[0]^'PA'", "eta[1]^'PA'")

# two different options for ordering the parameters
# levels &lt;- c("beta[0]^'NA'", "beta[1]^'NA'", "beta[0]^'PA'", "beta[1]^'PA'", "eta[0]^'NA'", "eta[1]^'NA'", "eta[0]^'PA'", "eta[1]^'PA'")
# levels &lt;- c("beta[0]^'NA'", "beta[0]^'PA'", "beta[1]^'NA'", "beta[1]^'PA'","eta[0]^'NA'", "eta[0]^'PA'", "eta[1]^'NA'", "eta[1]^'PA'")

rho &lt;-
  posterior_summary(melsm.4) %&gt;% 
  data.frame() %&gt;% 
  rownames_to_column("param") %&gt;% 
  filter(str_detect(param, "cor_")) %&gt;% 
  mutate(param = str_remove(param, "cor_record_id__")) %&gt;% 
  separate(param, into = c("left", "right"), sep = "__") %&gt;% 
  mutate(
    left = case_when(
      left == "NAstd_Intercept"       ~ "beta[0]^'NA'",
      left == "NAstd_day01"           ~ "beta[1]^'NA'",
      left == "sigma_NAstd_Intercept" ~ "eta[0]^'NA'",
      left == "sigma_NAstd_day01"     ~ "eta[1]^'NA'",
      left == "PAstd_Intercept"       ~ "beta[0]^'PA'",
      left == "PAstd_day01"           ~ "beta[1]^'PA'",
      left == "sigma_PAstd_Intercept" ~ "eta[0]^'PA'",
      left == "sigma_PAstd_day01"     ~ "eta[1]^'PA'"
      ),
    right = case_when(
      right == "NAstd_Intercept"       ~ "beta[0]^'NA'",
      right == "NAstd_day01"           ~ "beta[1]^'NA'",
      right == "sigma_NAstd_Intercept" ~ "eta[0]^'NA'",
      right == "sigma_NAstd_day01"     ~ "eta[1]^'NA'",
      right == "PAstd_Intercept"       ~ "beta[0]^'PA'",
      right == "PAstd_day01"           ~ "beta[1]^'PA'",
      right == "sigma_PAstd_Intercept" ~ "eta[0]^'PA'",
      right == "sigma_PAstd_day01"     ~ "eta[1]^'PA'"
    )
  ) %&gt;% 
  mutate(label = formatC(Estimate, digits = 2, format = "f") %&gt;% str_replace(., "0.", ".")) %&gt;% 
  mutate(left  = factor(left, levels = levels),
         right = factor(right, levels = levels)) %&gt;% 
  mutate(right = fct_rev(right))

rho %&gt;% 
  full_join(rename(rho, right = left, left = right),
            by = c("left", "right", "Estimate", "Est.Error", "Q2.5", "Q97.5", "label")) %&gt;%
  ggplot(aes(x = left, y = right)) +
  geom_tile(aes(fill = Estimate)) +
  geom_hline(yintercept = 4.5, color = "#100F14") +
  geom_vline(xintercept = 4.5, color = "#100F14") +
  geom_text(aes(label = label),
            family = "Courier", size = 3) +
  scale_fill_gradient2(expression(rho),
                       low = "#59708b", mid = "#FCF9F0", high = "#A65141", midpoint = 0,
                       labels = c(-1, "", 0, "", 1), limits = c(-1, 1)) +
  scale_x_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe, position = "top") +
  scale_y_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe) +
  theme(axis.text = element_text(size = 12),
        axis.ticks = element_blank(),
        legend.text = element_text(hjust = 1))
```

---
## Missing data
How to solve? 
1. Listwise
2. Estimation algorithm eg FIML
3. Multiple Imputation
4. Bayesian

---
## brms and multiple imputation

Each missing value is not imputed N times leading to a total of N fully imputed data sets. The model is fitted to each data sets separately and results are pooled across models.


```r
library(mice)
#multivariate imputation by chained equations
```

---

```r
data("nhanes")
#National Health and Nutrition Examination Survey
nhanes
```

```
##    age  bmi hyp chl
## 1    1   NA  NA  NA
## 2    2 22.7   1 187
## 3    1   NA   1 187
## 4    3   NA  NA  NA
## 5    1 20.4   1 113
## 6    3   NA  NA 184
## 7    1 22.5   1 118
## 8    1 30.1   1 187
## 9    2 22.0   1 238
## 10   2   NA  NA  NA
## 11   1   NA  NA  NA
## 12   2   NA  NA  NA
## 13   3 21.7   1 206
## 14   2 28.7   2 204
## 15   1 29.6   1  NA
## 16   1   NA  NA  NA
## 17   3 27.2   2 284
## 18   2 26.3   2 199
## 19   1 35.3   1 218
## 20   3 25.5   2  NA
## 21   1   NA  NA  NA
## 22   1 33.2   1 229
## 23   1 27.5   1 131
## 24   3 24.9   1  NA
## 25   2 27.4   1 186
```

---

```r
nhanes.imp &lt;- mice(nhanes, m = 10)
```

```
## 
##  iter imp variable
##   1   1  bmi  hyp  chl
##   1   2  bmi  hyp  chl
##   1   3  bmi  hyp  chl
##   1   4  bmi  hyp  chl
##   1   5  bmi  hyp  chl
##   1   6  bmi  hyp  chl
##   1   7  bmi  hyp  chl
##   1   8  bmi  hyp  chl
##   1   9  bmi  hyp  chl
##   1   10  bmi  hyp  chl
##   2   1  bmi  hyp  chl
##   2   2  bmi  hyp  chl
##   2   3  bmi  hyp  chl
##   2   4  bmi  hyp  chl
##   2   5  bmi  hyp  chl
##   2   6  bmi  hyp  chl
##   2   7  bmi  hyp  chl
##   2   8  bmi  hyp  chl
##   2   9  bmi  hyp  chl
##   2   10  bmi  hyp  chl
##   3   1  bmi  hyp  chl
##   3   2  bmi  hyp  chl
##   3   3  bmi  hyp  chl
##   3   4  bmi  hyp  chl
##   3   5  bmi  hyp  chl
##   3   6  bmi  hyp  chl
##   3   7  bmi  hyp  chl
##   3   8  bmi  hyp  chl
##   3   9  bmi  hyp  chl
##   3   10  bmi  hyp  chl
##   4   1  bmi  hyp  chl
##   4   2  bmi  hyp  chl
##   4   3  bmi  hyp  chl
##   4   4  bmi  hyp  chl
##   4   5  bmi  hyp  chl
##   4   6  bmi  hyp  chl
##   4   7  bmi  hyp  chl
##   4   8  bmi  hyp  chl
##   4   9  bmi  hyp  chl
##   4   10  bmi  hyp  chl
##   5   1  bmi  hyp  chl
##   5   2  bmi  hyp  chl
##   5   3  bmi  hyp  chl
##   5   4  bmi  hyp  chl
##   5   5  bmi  hyp  chl
##   5   6  bmi  hyp  chl
##   5   7  bmi  hyp  chl
##   5   8  bmi  hyp  chl
##   5   9  bmi  hyp  chl
##   5   10  bmi  hyp  chl
```

---

```r
nhanes.imp
```

```
## Class: mids
## Number of multiple imputations:  10 
## Imputation methods:
##   age   bmi   hyp   chl 
##    "" "pmm" "pmm" "pmm" 
## PredictorMatrix:
##     age bmi hyp chl
## age   0   1   1   1
## bmi   1   0   1   1
## hyp   1   1   0   1
## chl   1   1   1   0
```
pre- dictive mean matching (pmm). 

More information, usually results in better imputations. 

---
### brm_multiple

works well with mice objects, but brm_multiple also takes any list of dataframes. Helpful if you use amelia or mi. 

```r
imp.1 &lt;- brm_multiple(family = gaussian,
                      bmi ~ age*chl, 
                      data = nhanes.imp,
                      cores = 4, 
                      file = "imp.1")
```


---
Pooling across models is trivial in a Bayesian framework but not in frequentist. 
40 Chains! 40k samples! (10 datasets + default 4 chains and 1k samples)

```r
summary(imp.1)
```

```
## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be
## careful when analysing the results! We recommend running more iterations and/or
## setting stronger priors.
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: bmi ~ age * chl 
##    Data: nhanes.imp (Number of observations: 25) 
## Samples: 40 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 40000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    15.72      7.10     1.47    29.68 1.04      588     2538
## age           0.73      4.28    -7.65     9.24 1.03      872     9434
## chl           0.09      0.04     0.01     0.17 1.05      488     2248
## age:chl      -0.02      0.02    -0.06     0.02 1.02     1096     3618
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     3.19      0.60     2.21     4.52 1.19      143      533
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---


```r
plot(imp.1)
```

![](grabbag_files/figure-html/unnamed-chunk-71-1.png)&lt;!-- --&gt;

---
## bayesian imputation within brms

1) Which variables contain missingness? 2) Which variables should predict missingness 3) what imputed variables are used as predictors

Taking care of this in brms ends up creating a multivariate model

```r
bform &lt;- bf(bmi | mi() ~ age * mi(chl)) +
  bf(chl | mi() ~ age) + set_rescor(FALSE)
imp.2 &lt;- brm(bform, data = nhanes, file = "imp.2")
```


---

```r
summary(imp.2)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: bmi | mi() ~ age * mi(chl) 
##          chl | mi() ~ age 
##    Data: nhanes (Number of observations: 25) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## bmi_Intercept    14.59      8.81    -2.45    32.60 1.00     1073     1764
## chl_Intercept   140.94     25.20    91.44   191.34 1.00     2426     2833
## bmi_age           2.13      5.53    -8.53    13.30 1.00      995     1646
## chl_age          28.93     13.45     1.36    55.04 1.00     2379     2657
## bmi_michl         0.09      0.05     0.00     0.18 1.00     1144     1730
## bmi_michl:age    -0.03      0.02    -0.08     0.02 1.00     1019     1668
## 
## Family Specific Parameters: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_bmi     3.35      0.80     2.18     5.27 1.00     1146     1683
## sigma_chl    40.47      7.72    28.55    58.30 1.00     2320     2861
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
### pro v con

Pros: Can use multilevel structure and complex non-linear relationships for the imputation of missing values, which is not achieved as easily in standard multiple imputation software

Cons: cannot impute discrete values within brms/Stan

---


```r
posterior_summary(imp.2)
```

```
##                        Estimate   Est.Error          Q2.5         Q97.5
## b_bmi_Intercept     14.59488641  8.81176462 -2.448680e+00   32.59832855
## b_chl_Intercept    140.93781168 25.20305154  9.143922e+01  191.34335044
## b_bmi_age            2.12652344  5.52999873 -8.526886e+00   13.29505916
## b_chl_age           28.92920155 13.44925130  1.363412e+00   55.03647826
## bsp_bmi_michl        0.09491185  0.04543537  2.837374e-03    0.18299883
## bsp_bmi_michl:age   -0.02752190  0.02480020 -7.870264e-02    0.02211829
## sigma_bmi            3.34561156  0.80459699  2.179160e+00    5.27371494
## sigma_chl           40.46789591  7.72047729  2.855365e+01   58.30256407
## Ymi_bmi[1]          28.11246823  4.64224760  1.896736e+01   37.24229912
## Ymi_bmi[3]          29.26761458  3.72829075  2.195424e+01   36.69163342
## Ymi_bmi[4]          23.79154580  4.10204618  1.580293e+01   32.04254142
## Ymi_bmi[6]          23.24487443  4.30626232  1.507441e+01   32.06240230
## Ymi_bmi[10]         26.72586313  4.28418130  1.831310e+01   35.36479446
## Ymi_bmi[11]         28.11011846  4.74439508  1.852274e+01   37.63763112
## Ymi_bmi[12]         26.86471159  4.03049184  1.894633e+01   34.66988290
## Ymi_bmi[16]         28.10233705  4.74450451  1.836223e+01   37.48192756
## Ymi_bmi[21]         28.17085005  4.73897620  1.837307e+01   37.18454748
## Ymi_chl[1]         169.25931215 42.29523119  8.169740e+01  251.73817251
## Ymi_chl[4]         227.28739466 45.52866436  1.390978e+02  315.26286984
## Ymi_chl[10]        197.97316131 43.47526835  1.090020e+02  284.49495968
## Ymi_chl[11]        169.59149463 42.14379521  8.676544e+01  252.94213525
## Ymi_chl[12]        199.38255877 41.27610739  1.177572e+02  280.67956121
## Ymi_chl[15]        177.11695542 32.43802339  1.102094e+02  239.87710160
## Ymi_chl[16]        169.48034204 43.10010724  8.433979e+01  254.43303157
## Ymi_chl[20]        231.38707637 42.23394598  1.454395e+02  313.94705371
## Ymi_chl[21]        170.08112459 42.95487739  8.440611e+01  258.45626112
## Ymi_chl[24]        229.55226399 41.99063063  1.461618e+02  311.95887048
## lp__              -203.10730660  5.12956336 -2.145705e+02 -194.10234548
```

---


```r
imp.2 %&gt;% 
  spread_draws(Ymi_chl[ID]) 
```

```
## # A tibble: 40,000 x 5
## # Groups:   ID [10]
##       ID Ymi_chl .chain .iteration .draw
##    &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;
##  1     1    153.      1          1     1
##  2     1    147.      1          2     2
##  3     1    144.      1          3     3
##  4     1    220.      1          4     4
##  5     1    247.      1          5     5
##  6     1    179.      1          6     6
##  7     1    198.      1          7     7
##  8     1    157.      1          8     8
##  9     1    213.      1          9     9
## 10     1    171.      1         10    10
## # … with 39,990 more rows
```

4000* * 10 (missing for chl) = 40,000
---

.pull-left[

```r
imp.2 %&gt;% 
  spread_draws(Ymi_chl[ID]) %&gt;% 
  ggplot(aes(x = Ymi_chl, 
             y = reorder(ID, Ymi_chl))) +
  stat_slab(fill = "black", alpha = 3/4, height = 1.6, slab_color = "black", slab_size = 1/4) +
  labs(x = "Chl imputed values", y = "IDs") +
  theme_ggdist()
```
]

.pull-right[

![](grabbag_files/figure-html/unnamed-chunk-77-1.png)&lt;!-- --&gt;


]

---

![](grabbag_files/figure-html/unnamed-chunk-78-1.png)&lt;!-- --&gt;


---
## Semester review

Three goals: 
1. Understand Bayesian estimation
2. Better understanding of linear models
3. To use brms and tidybayes to explore the posterior

---
## "Bayesian analysis is just counting"

MCMC is just counting the ways that your data are consistent with different parameter values. Parameter values that are more consistent with your data will emerge more in the posterior samples. 

---
## "Bayesian inference is reallocation of credibility across parameter values"

A `prior` distribution can be thought of as previous counts/parameters. We merely update this distribution through the collection of new data. 

Regardless of whether the prior is made up, reflects past results, or the result of 1 data point in our new sample, each posterior we have can then be used as a prior for a new analysis. This is `bayesian updating` where you can always improve your model. 

No deep distinction between prior and posterior

---
## Bayesian models are generative

Generative models allow one to `generate` new data. Most of the linear models you have been working with are descriptive, and do not incorporate all of the components needed to generate the same data. SR talks about these as pushing data forward and backwards. Forwards to get parameters, and backwards to simulate data. We do this push and pull when we want to evaluate models. 

---
## Model statements

We now have a language to describe a full generative model. Even for a simple regression we now are able to describe all components that lead to the generation of the data. The spread of your DV, sigma, plays a large role of this but is usually not even examined, let alone modeled with standard frequentest frameworks

P_Height_i ~ Normal( `\(\mu_i\)` , `\(\sigma\)` )  
`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` ( `\(\text{C_Height}_i\)` - `\({\overline{\mbox{C_Height}}}\)` ) 
`\(\beta_0\)` ~ Normal(68, 5)
`\(\beta_1\)` ~ Normal(0, 5)  
`\(\sigma\)`  ~ HalfCauchy(0,1)

---
## Propogation of uncertainty

One of the key components of Bayesian (and generative modeling) is that we include all of the uncertainty in our estimates. With standard frequentist we do not, as we use our best guess (the regression line) and sigma -- and do not incorporate uncertainty in the estimation of our parameter.

Uncertainty estimates baked into the model also allow for easy calculation of credible intervals/bands. Just count up the samples to calculate. Within frequentist you have to use equations that make assumptions about data distribution and/or bootstrap.  

---
## Linear models all the way down (and up)

Anova, logistic, etc are all parts of the glm. If you learn the glm you can model anything. 


---
## brms

Analysis options change all the time. brms (and tidyverse) make the transition to bayesian models easier. But it doesn't have to be this way. ~40 Bayesian packages within R, Stan (within R or not), Python (Stan, PyMC), julia (Turing), etc, etc. 

---
## Terms

Prior, Likelihood, Posterior. 
Grid estimation, Credible Interval, Highest Posterior Density Interval, MAP, Posterior Predictive Distribution, prior predictive distribution, fitted values/predicted values, index variables, maximum entropy, loo, waic, Rope, zero-inflated models, ordinal regression models, monotonic regression models, hyperprior, shrinkage, MrP, multivariate and distributional models. 





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
